\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{mathptmx}
\usepackage[colorlinks, citecolor=black, linkcolor=black, %
filecolor=black, urlcolor=blue]{hyperref}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algpseudocode}

\newcommand{\mb}{\mathbf}
\newcommand{\p}{\partial}
\newcommand{\mbs}{\boldsymbol}
\newcommand{\f}{\frac}

\title{ParOpt: A parallel library of large-scale optimization algorithms}
\author{Graeme J. Kennedy \and Yicong Fu}
\date{}

\begin{document}

\maketitle

\section{Introduction}

ParOpt is a parallel gradient-based optimization library implemented in C++ and is intended for solving large-scale constrained optimization problems.
ParOpt can be applied to general purpose optimization problems, but is often used for large-scale topology optimization.
The constraints in ParOpt fall into one of two categories: (1) constraints with full dependence on the design vector such that the constraint Jacobian is fully populated, or (2) constraints that have a specific sparse structure, described below, that enables them to be grouped independently.
All operations in ParOpt use distributed design vectors and almost all computations are performed in parallel, with a small number of factorization operations performed on small dense matrices in serial.
ParOpt can optionally use information from Hessian-vector products to accelerate convergence.
Within the Hessian-vector product mode, inexact solutions of the KKT system are used where the tolerances are determined using the Eisenstat--Walker forcing terms.

ParOpt utilizes both a C++ and a python-level interface.
The python-level interface is generated using cython.
Call-backs from C++ to python are implemented using direct memory access into numpy arrays.
Some care must be exercised when setting or reading values from arrays passed to python-level functions so as not to inadvertently set gradient or Jacobian values into a copied vector.
Furthermore, the design variable vector passed during callbacks is used by ParOpt, so modification of design vector will produce undesirable results.
ParOpt uses an abstract problem interface with an abstract vector class that can be implemented by application-specific methods.
This enables the use of externally-defined vectors, as long as basic vector-vector and vector-scalar operations are defined.
A default ParOptVec class is implemented to provide generic functionality.

The following document is divided into two sections: (1) a high-level description of the algorithms implemented in ParOpt, and (2) a detailed description of the implementation of ParOpt.

\section{Interior point algorithm}

ParOpt implements an interior-point method to solve optimization problems formulated as follows:
\begin{equation}
  \label{eqn:opt-problem}
  \begin{aligned}
    \min_{\mb{x}} \qquad & f(\mb{x}) \\
    \text{such that} \qquad
    & \mb{c}(\mb{x}) \ge 0 \\
    & \mb{c}_{w}(\mb{x}) \ge 0 \\
    & \mb{l} \le \mb{x} \le \mb{u}
  \end{aligned}
\end{equation}
Here, $\mb{c}(\mb{x})$ are the dense constraints, and $\mb{c}_{w}(\mb{x})$ are the sparse constraints.
The sparse constraint Jacobian, $\mb{A}_{w} = \nabla_{x} \mb{c}_{w}(\mb{x})$ must have a structure such that the matrix
\begin{equation*}
  \mb{D} = \mb{A}_{w}\mb{S}\mb{A}_{w}^{T}
\end{equation*}
is a block-diagonal matrix whenever $\mb{S}$ is a diagonal matrix.
This structure arises in many topology and multimaterial optimization problems that employ weighting constraints for each element within the problem.

An interior point algorithm approximately solves a sequence of barrier problems that are designed to approach the true constrained minimizer in the limit.
The barrier problem is formed by adding inequality constraints to the objective through a log barrier penalty function.
This barrier function is designed to keep the iterates strictly in the interior of the feasible region.
The barrier problem corresponding to~\eqref{eqn:opt-problem} is the following
\begin{equation}
  \label{eqn:barrier-problem}
  \begin{aligned}
    \min_{\mb{x}, \mb{s}, \mb{t}, \mb{s}_{w}} \qquad &  \varphi(\mb{x}, \mb{s}, \mb{t}, \mb{s}_{w}; \mu) =
    f(\mb{x}) + \gamma_{t}^{T}\mb{t} + \gamma_{s}^{T}\mb{s} - \mu \left[ \log \mb{s} + \log \mb{t} + \log \mb{s}_{w} + \log (\mb{x} - \mb{l}) + \log (\mb{u} - \mb{x})  \right] \\
    \text{such that} \qquad & \mb{c}(\mb{x}) = \mb{s} - \mb{t} \\
    & \mb{c}_{w}(\mb{x}) = \mb{s}_{w} \\
  \end{aligned}
\end{equation}
where $\mb{s}$, $\mb{t}$ and $\mb{s}_{w}$ are slack variables associated with the dense and sparse constraints, respectively.
The function $\log$ is the component-wise sum of the logarithms of the vector components, i.e. $\log \mb{s} = \sum_{i} \ln s_{i}$.
As the barrier parameter, $\mu$, decreases, the minimizer of the barrier problem~\eqref{eqn:barrier-problem} approaches the KKT solution.

The barrier problem~\eqref{eqn:barrier-problem} is related to a set of perturbed KKT conditions for the optimization problem~\eqref{eqn:opt-problem}.
Introducing dual variables for the dense constraints $\mb{z}$, the sparse constraints $\mb{z}_{w}$, and the lower and upper bounds, $\mb{z}_{l}$, and $\mb{z}_{u}$, the perturbed KKT conditions can be written as follows:
%
\begin{equation}
  \label{eqn:perturbed-kkt}
  \begin{aligned}
    \mb{r}_{x} \triangleq     & \mb{g} - \mb{A}^{T}\mb{z} - \mb{A}^{T}_{w} \mb{z}_{w} - \mb{z}_{l} + \mb{z}_{u} = 0 \\
    \mb{r}_{s} \triangleq     & \gamma_{s} + \mb{z} - \mb{z}_{s} = 0 \\
    \mb{r}_{t} \triangleq     & \gamma_{t} - \mb{z} - \mb{z}_{t} = 0 \\
    \mb{r}_{c} \triangleq     & \mb{c} - \mb{s} + \mb{t} = 0 \\
    \mb{r}_{c_{w}} \triangleq & \mb{c}_{w} - \mb{s}_{w} = 0 \\
    \mb{r}_{z_{s}} \triangleq & \mb{S} \mb{z}_{s} - \mu \mb{e} = 0 \\
    \mb{r}_{z_{t}} \triangleq & \mb{T}\mb{z}_{t} - \mu \mb{e} = 0 \\
    \mb{r}_{z_{w}} \triangleq & \mb{S}_{w} \mb{z}_{w} - \mu \mb{e} = 0 \\
    \mb{r}_{z_{l}} \triangleq & (\mb{X} - \mb{L})\mb{z}_{l} - \mu \mb{e} = 0 \\
    \mb{r}_{z_{u}} \triangleq & (\mb{U} - \mb{X})\mb{z}_{u} - \mu \mb{e} = 0
  \end{aligned}
\end{equation}
Here, the gradient of the objective function is $\mb{g} = \nabla_{x} f(\mb{x})$ and the Jacobians of the constraints are $\mb{A} = \nabla_{x} \mb{c}(\mb{x})$ and $\mb{A}_{w} = \nabla_{x} \mb{c}_{w}(\mb{x})$.

At each step of the optimization algorithm, ParOpt computes an update $\mb{p}$ to the design variables, slacks, and multipliers, based on either an inexact or an approximate Newton step, which can be written as follows:
%
\begin{equation*}
  \mb{K} \mb{p} = - \mb{r},
\end{equation*}
where $\mb{K}$ is either the exact Jacobian or an approximate Jacobian of the perturbed KKT system~\eqref{eqn:perturbed-kkt}.
When a quasi-Newton method is used the Jacobian is approximate, and when Hessian-vector products are used the Jacobian is exact, but the linear system is solved inexactly.

In the quasi-Newton mode, the matrix $\mb{K}_{B} \approx \mb{K}$ is an approximate Jacobian due to the use of a Hessian approximation $\mb{B} \approx \mb{H} \triangleq \nabla_{x}^{2} \left(f(\mb{x}) - \mb{z}^{T}\mb{c}(\mb{x}) - \mb{z}_{w}^{T}\mb{c}_{w}(\mb{x}) \right)$.
The approximate KKT matrix is
%
\begin{equation}
  \label{eqn:kkt-update}
  \mb{K}_{B}\mb{p} =
  \begin{bmatrix}
    \mb{B}     & 0          & 0          & 0          & -\mb{A}^{T} & 0       & 0       & -\mb{A}^{T}_{w} & -\mb{I} & \mb{I} \\
    0          & 0          & 0          & 0          & \mb{I}      & -\mb{I} & 0       & 0               & 0       & 0 \\
    0          & 0          & 0          & 0          & -\mb{I}     & 0       & -\mb{I} & 0               & 0       & 0 \\
    \mb{A}     & -\mb{I}    & \mb{I}     & 0          & 0           & 0       & 0       & 0               & 0       & 0 \\
    \mb{A}_{w} & 0          & 0          & -\mb{I}    & 0           & 0       & 0       & 0               & 0       & 0 \\
    0          & \mb{Z}_{s} & 0          & 0          & 0           & \mb{S}  & 0       & 0               & 0       & 0 \\
    0          & 0          & \mb{Z}_{t} & 0          & 0           & 0       & \mb{T}  & 0               & 0       & 0 \\
    0          & 0          & 0          & \mb{Z}_{w} & 0           & 0       & 0       & \mb{S}_{w}      & 0       & 0 \\
    \mb{Z}_{l} & 0          & 0          & 0          & 0           & 0       & 0       & 0               & (\mb{X} - \mb{L}) & 0 \\
    -\mb{Z}_{u}& 0          & 0          & 0          & 0           & 0       & 0       & 0               & 0 & (\mb{U} - \mb{X}) \\
  \end{bmatrix}
  \begin{bmatrix}
    \mb{p}_{x} \\
    \mb{p}_{s} \\
    \mb{p}_{t} \\
    \mb{p}_{s_{w}} \\
    \mb{p}_{z} \\
    \mb{p}_{z_{s}} \\
    \mb{p}_{z_{t}} \\
    \mb{p}_{z_{w}} \\
    \mb{p}_{z_{l}} \\
    \mb{p}_{z_{u}}
  \end{bmatrix} = -\mb{r}.
\end{equation}

ParOpt uses quasi-Newton Hessian approximations based either on compact limited-memory BFGS or compact limited-memory SR1 updates~\citep{Byrd:1994:quasi-Newton-LBFGS}.
Compact representations of limited-memory quasi-Newton approximations take the form
%
\begin{equation*}
  \mb{B} = b_0\mb{I} - \mb{W}\mb{M}\mb{W}^{T},
\end{equation*}
where $b_0$ is a scalar, $\mb{M}$ is a small matrix and $\mb{W}$ is a matrix with a small number of columns that is stored as a series of vectors.
The form of these matrices depends on whether the limited-memory BFGS or SR1 technique is used.
An exact solution to the update step $\mb{p}$ can be obtained by using the compact representation in conjunction with the Sherman-Morrison-Woodbury formula.

ParOpt approximately solves the perturbed KKT equations~\eqref{eqn:perturbed-kkt} for a sequence of barrier parameters $\mu_{k}$ such that $\mu_{k} \rightarrow 0$ for $k \rightarrow \infty$.
ParOpt uses a monotone approach~\citet{Fiacco:McCormick:1990} in which the barrier parameter is maintained at a fixed value and reduced only after a barrier-problem convergence criterion is satisfied.
The barrier parameter criterion is that the infinity norm of the solution vector must be reduced below a factor of the barrier parameter itself
%
\begin{equation}
  \label{eqn:barrier-stopping-criterion}
  ||\mb{r}||_{\infty}  \le 10 \mu_{k}.
\end{equation}
After the barrier criterion is satisfied, the parameter is modified using the expression $\mu_{k+1} \leftarrow \min \{ \; \theta \mu_{k},\; \mu_{k}^{\beta}\;\}$ for $\beta \in (1, 2]$.

\subsection{Merit function and line search}

The interior-point method implemented in ParOpt uses a line search method that guarantees a sufficient decrease of a merit function at each iteration.
The line search is based on the following $\ell_{2}$ merit function:
%
\begin{equation}
  \label{eqn:merit-function}
  \begin{aligned}
    \phi(\alpha) = & \varphi(\mb{x} + \alpha\mb{p}_{x}^{s}, \mb{s} + \alpha\mb{p}_{s}^{s}, \mb{t} + \alpha\mb{p}_{t}^{s}, \mb{s}_{w}^{s} + \alpha\mb{p}^{s}_{s_{w}}; \mu) + \\
    & \nu ||\mb{c}(\mb{x} + \alpha\mb{p}_{s}^{s}) - \mb{s} + \mb{t}  - \alpha (\mb{p}_{s}^{s} - \mb{p}_{t}^{s})||_{2} +
    \nu ||\mb{c}_{w}(\mb{x} + \alpha\mb{p}^{s}_{s_{w}}) - \mb{s}_{w} - \alpha\mb{p}_{s_{w}}^{s}||_{2},
  \end{aligned}
\end{equation}
%
where $\mb{p}^{s}$ is the KKT update vector $\mb{p}$ scaled to ensure that the primal variables remain strictly within the feasible region and so that the dual variables remain positive.
The penalty parameter $\nu$ is selected to ensure a sufficiently negative descent direction, such that $\phi'(0)$ is sufficiently negative~\citep{Nocedal.Wright}.
At each step ParOpt uses a line search that seeks a point that satisfies the Armijo sufficient decrease condition:
%
\begin{equation*}
  \phi(\alpha) < \phi(0) + c_{1} \alpha \phi'(0),
\end{equation*}
where we typically choose $c_{1} = 10^{-3}$. If a step is unsuccessful, we select the next step using a quadratic interpolation based on the initial point and slope of the merit function along the search direction, as well as the most recent merit function value.
Since $\phi'(0)$ is negative, and $\phi(\alpha) \ge \phi(0) + c_{1} \alpha \phi'(0)$, this sequence of step lengths is decreasing.

To ensure that the design variables remain within bounds and that the dual and slack variables remain sufficiently positive, ParOpt uses a fraction-to-the-boundary rule, such that
%
\begin{equation*}
  \begin{aligned}
    \alpha_{x} & = \max\left\{ \alpha \in (0, 1]\;|\;
      \mb{x} + \alpha \mb{p}_{x} - \mb{l} \ge (1 - \tau)(\mb{x} - \mb{l}) \right\}, \\
      %
      \alpha_{z} & = \max\left\{ \alpha \in (0, 1]\;|\;
      \mb{z}_{s} + \alpha \mb{p}_{s} \ge (1 - \tau)\mb{z}_{s} \right\},
  \end{aligned}
\end{equation*}
%
with analogous expressions for the remaining components of the step length vector $\mb{p}$.
Note that the sign of $\mb{z}$ is not directly constrained since it is treated as a multiplier for an equality constraint.
The $\alpha_{x}$ and $\alpha_{z}$ parameters are then used to compute the step $\mb{p}^{s}$ such that $\mb{p}_{x}^{s} = \alpha_{x} \mb{p}_{x}$ and $\mb{p}_{z} = \alpha_{z}\mb{p}_{z}$.
Note that $\alpha_{x}$ is the step length for the design and slack variables, and $\alpha_{z}$ is the step length for all multipliers.
Following~\citet{Wachter:2006:IPOPT}, ParOpt sets the parameter $\tau$ as follows
%
\begin{equation*}
  \tau = \max(0.95, 1 - \mu).
\end{equation*}
To avoid situations in which there is a large discrepancy between the step lengths, a check is imposed on $\alpha_{x}$ and $\alpha_{z}$ such that if $\alpha_{x} >> \alpha_z$, ParOpt truncates the difference between the step lengths such that
%
\begin{equation*}
  \alpha_x = \max(\min(\alpha_x, 100\alpha_z), \alpha_z/100),
\end{equation*}
otherwise if $\alpha_z > \alpha_{x}$, we set:
\begin{equation*}
  \alpha_z = \max(\min(\alpha_z, 100\alpha_x), \alpha_x/100).
\end{equation*}
Note that this modification only has an effect if the difference in step lengths exceeds $100$.
This modification does not interfere with the asymptotic convergence behavior of the algorithm and enables faster recovery from poor steps early in the optimization.

\subsection{Solving the approximate KKT system with a compact quasi-Newton Hessian}

Within ParOpt, the single most computationally expensive operation at each iteration of the optimization algorithm is the solution of the linearized KKT system obtained from the perturbed KKT conditions~\eqref{eqn:perturbed-kkt}.
The following section presents an overview of the methods used to solve this linear system in a computationally efficient manner in parallel.

To derive the proposed solution procedure, we first express the linearized KKT matrix as a combination of two matrices which take the form:
%
\begin{equation}
  \label{eqn:ip-newton-update}
  \mb{K}_{B} \mb{p} = \left[ \mb{K}_{0} + \mb{Y} \mb{M} \mb{Y}^{T} \right] \mb{p} = - \mb{r}
\end{equation}
where the matrix $\mb{Y}$ is
%
\begin{equation*}
  \mb{Y}^{T} = \begin{bmatrix} \mb{W}^T & 0 & 0 & 0 &
    0 & 0 & 0 & 0 & 0 & 0 \end{bmatrix},
\end{equation*}
and the matrices $\mb{W}$ and $\mb{M}$ are from the compact BFGS representation.
Note that the terms in $\mb{K}_{0}$ represent the diagonal term $b_{0}$ from the compact BFGS representation and all other first-order terms from the linearized KKT system.

An exact solution to the linear system~\eqref{eqn:ip-newton-update} can be obtained using the Sherman--Morrison--Woodbury formula.
This formula leads to the following expression for the update $\mb{p}$:
\begin{equation*}
  \mb{p} = \mb{K}^{-1}_{0} \mb{Y} \mb{C}^{-1} \mb{Y}^{T} \mb{K}_{0}^{-1} \mb{r} - \mb{K}_{0}^{-1} \mb{r}
\end{equation*}
where the matrix $\mb{C} \in \mathbb{R}^{2m \times 2m}$ is given as
follows:
\begin{equation*}
  \mb{C} = \mb{Y}^{T} \mb{K}_{0}^{-1}\mb{Y} + \mb{M}^{-1}.
\end{equation*}
A solution of the linear system~\eqref{eqn:ip-newton-update} can be obtained from the solution of $2m+1$ linear systems of the form $\mb{K}_{0}\mb{y} = \mb{b}$.
Furthermore, if the matrix $\mb{Y}$ is stored as a series of column vectors, then the operations required to compute the solution consist of operations with small matrices of size $\mathcal{O}(m)$, parallel vector-vector products, and the application of $\mb{K}_{0}^{-1}$.
Since vector-vector operations parallelize efficiently for distributed vectors, and the small matrix operations normally constitute a small contribution to the overall computational time, we concentrate on the parallel solution of systems of the form $\mb{K}_{0} \mb{p} =
\mb{b}$.
Note that this refer to the linear system $\mb{K}_{0} \mb{p} = \mb{b}$ as the diagonal KKT matrix since the Hessian term in the matrix $\mb{K}_{0}$ is replaced by a diagonal matrix, $\mb{B} = b_{0}\mb{I}$.

\subsection{Parallel solution of the diagonal KKT system}

The diagonal KKT system $\mb{K}_{0} \mb{p} = \mb{b}$ is solved in parallel through a series of variable eliminations.
In general, this method can be susceptible to numerical cancellation, however, experience has shown that this method produces remarkably accurate steps, even for very small values of the barrier parameter.
This can be attributed to the structure of the constraint Jacobians.
The sequence of variable eliminations produces a linear system for the Lagrange multipliers of the dense constraints.
The Schur-complement matrices that are produced during the elimination process can be precomputed and stored.
The computations during the elimination process can be reduced to a sequence of vector-vector operations and can therefore be implemented efficiently in parallel.
Since each operation can be performed in parallel, with a small number of dense matrix operations on matrices of size $\mathcal{O}(m)$, the entire solution procedure scales efficiently.

The solution procedure begins by obtaining the solution for the slack variables and lower and upper Lagrange multiplier bound variables as follows:
%
\begin{equation}
  \label{eqn:slack-var-elim}
  \begin{aligned}
    \mb{p}_{z_{l}} &= (\mb{X} - \mb{L})^{-1}(\mb{b}_{z_{l}} - \mb{Z}_{l} \mb{p}_{x} ), \\
    \mb{p}_{z_{u}} &= (\mb{U} - \mb{X})^{-1}(\mb{b}_{z_{u}} + \mb{Z}_{u} \mb{p}_{x} ), \\
    \mb{p}_{z} - \mb{p}_{z_{s}} &= \mb{b}_{s}, \\
    -\mb{p}_{z} - \mb{p}_{z_{t}} &= \mb{b}_{t}, \\
    \mb{p}_{s} &= \mb{Z}_{s}^{-1}(\mb{b}_{z_{s}} - \mb{S}\mb{p}_{z_{s}}) = \mb{Z}_{s}^{-1}(\mb{b}_{z_{s}} - \mb{S}(\mb{p}_{z} - \mb{b}_{s})), \\
    \mb{p}_{t} &= \mb{Z}_{t}^{-1}(\mb{b}_{z_{t}} - \mb{T}\mb{p}_{z_{t}}) = \mb{Z}_{t}^{-1}(\mb{b}_{z_{t}} + \mb{T}(\mb{p}_{z} + \mb{b}_{t})),, \\
    \mb{p}_{s_{w}} &= \mb{Z}_{w}^{-1}(\mb{b}_{s_{w}} - \mb{S}_{w} \mb{p}_{z_{w}}),
  \end{aligned}
\end{equation}
Next, using the first three equations gives
\begin{equation}
  \label{eqn:kkt-first-elim}
  \begin{aligned}
    b_{0} \mb{p}_{x} - \mb{A}^{T}\mb{p}_{z} - \mb{A}_{w}^{T}\mb{p}_{z_{w}} - \mb{p}_{z_{l}} + \mb{p}_{z_{u}} & = \mb{b}_{x}, \\
    \mb{A} \mb{p}_{x} - \mb{p}_{s} + \mb{p}_{t} &= \mb{b}_{c}, \\
    \mb{A}_{w} \mb{p}_{x} - \mb{p}_{s_{w}} & = \mb{b}_{z_{w}}.
  \end{aligned}
\end{equation}
Substituting the expressions for the slack and Lagrange multiplier updates~\eqref{eqn:slack-var-elim} into the expression for the first three linearized KKT conditions~\eqref{eqn:kkt-first-elim} yields the following
%
\begin{equation}
  \label{eqn:kkt-x-lagrange}
  \begin{aligned}
    \mb{D} \mb{p}_{x} - \mb{A}^{T}\mb{p}_{z} - \mb{A}_{w}^{T}\mb{p}_{z_{w}} &=
    \mb{b}_{x} + (\mb{X} - \mb{L})^{-1}\mb{b}_{z_{l}} - (\mb{U} - \mb{X})^{-1}\mb{b}_{z_{u}}, \\
    %
    \mb{A}\mb{p}_{x} + (\mb{Z}_{s}^{-1}\mb{S} + \mb{Z}_{t}^{-1}\mb{T}) \mb{p}_{z} &=
    \mb{b}_{c} + \mb{Z}_{s}^{-1}(\mb{b}_{z_{s}} + \mb{S}\mb{b}_{s}) - \mb{Z}_{t}^{-1}(\mb{b}_{z_{t}} + \mb{T}\mb{b}_{t})\\
    %
    \mb{A}_{w} \mb{p}_{x} + \mb{Z}_{w}^{-1}\mb{S}_{w} \mb{p}_{z_{w}} & =
    \mb{b}_{c_{w}} + \mb{Z}_{w}^{-1}\mb{b}_{s_{w}} \\
    \end{aligned}
\end{equation}
where the diagonal matrix $\mb{D}$ is defined as follows:
\begin{equation*}
  \mb{D} = \left[ b_{0} \mb{I} + (\mb{X} - \mb{L})^{-1}\mb{Z}_{l} + (\mb{U} - \mb{X})^{-1}\mb{Z}_{u} \right].
\end{equation*}
%
Finally, $\mb{p}_{x}$ can be eliminated for $\mb{p}_{z}$ and $\mb{p}_{z_{w}}$ as follows
\begin{equation}
  \begin{aligned}
    (\mb{Z}_{w}^{-1}\mb{S}_{w} + \mb{A}_{w}\mb{D}^{-1}\mb{A}_{w}^{T})\mb{p}_{z_{w}} + \mb{A}_{w}\mb{D}^{-1}\mb{A}^{T}\mb{p}_{z} &= \mb{d}_{z_{w}}, \\
    %
    \mb{A}\mb{D}^{-1}\mb{A}_{w}^{T} \mb{p}_{z_{w}} + (\mb{Z}_{s}^{-1}\mb{S} + \mb{Z}_{t}^{-1}\mb{T} + \mb{A}\mb{D}^{-1}\mb{A}^{T})\mb{p}_{z} &= \mb{d}_{z}, \\
  \end{aligned}
\end{equation}
%
The right hand sides, $\mb{d}_{z_{w}}$ and $\mb{d}_{z}$ are
\begin{equation*}
  \begin{aligned}
    \mb{d}_{z_{w}} &\triangleq \mb{b}_{c_{w}} + \mb{Z}_{w}^{-1}\mb{b}_{s_{w}} - \mb{A}_{w}\mb{D}^{-1}\left(\mb{b}_{x} + (\mb{X} - \mb{L})^{-1}\mb{b}_{z_{l}} - (\mb{U} - \mb{X})^{-1}\mb{b}_{z_{u}} \right), \\
    \mb{d}_{z} &\triangleq \mb{b}_{c} + \mb{Z}_{s}^{-1}(\mb{b}_{z_{s}} + \mb{S}\mb{b}_{s}) - \mb{Z}_{t}^{-1}(\mb{b}_{z_{t}} + \mb{T}\mb{b}_{t}) - \mb{A}\mb{D}^{-1}\left(\mb{b}_{x} + (\mb{X} - \mb{L})^{-1}\mb{b}_{z_{l}} - (\mb{U} - \mb{X})^{-1}\mb{b}_{z_{u}} \right). \\
  \end{aligned}
\end{equation*}
%
Finally, defining the matrix $\mb{E} \triangleq (\mb{Z}_{w}^{-1}\mb{S}_{w} + \mb{A}_{w}\mb{D}^{-1}\mb{A}_{w}^{T})$, which is block diagonal, and introducing $\mb{F} \triangleq \mb{A}\mb{D}^{-1}\mb{A}_{w}^{T}$, gives the following equation
\begin{equation*}
  \left[\mb{Z}_{s}^{-1}\mb{S} + \mb{Z}_{t}^{-1}\mb{T} + \mb{A}\mb{D}^{-1}\mb{A}^{T} - \mb{F}\mb{E}^{-1}\mb{F}^{T} \right]\mb{p}_{z} = \mb{d}_{z} - \mb{F}\mb{E}^{-1} \mb{d}_{z_{w}}
\end{equation*}

This solution procedure is invoked each time a solution of the form $\mb{K}_{0}\mb{p} = \mb{b}$ is required.

\subsection{Inexact Hessian-vector product mode}

ParOpt can also use Hessian-vector products to accelerate convergence.
This method is designed for convex optimization problems where the Hessian is positive semi-definite and the reduced Hessian is positive definite.
This solution phase is entered only after the residuals of the KKT equations are satisfied to a user-specified tolerance.
The exact Hessian phase employs an inexact Newton--Krylov method driven with the Eisenstat--Walker forcing parameters.
The inexact solution of the lineaized KKT system is obtained using right-preconditioned GMRES.
The convergence criteria within GMRES is modified to include conditions that enforce a descent direction for a line search method.
The preconditioner for the system of equations is the quasi-Newton approximation $\mb{K}_{B}$.
The product of the Jacobian and precondition is
%
\begin{equation}
  \begin{aligned}
    \mb{K}\mb{K}_{B}^{-1} & = \left(\mb{K}_{B} +
    \mb{N}\left(\mb{H} - \mb{B}\right)\mb{N}^{T}\right)\mb{K}_{B}^{-1} \\
    %
    & = \mb{I} + \mb{N} \left(\mb{H} - \mb{B} \right) \mb{N}^{T} \mb{K}_{B}^{-1}
  \end{aligned}
\end{equation}
Here $\mb{N}$ is a matrix that consists of an identity in the $\mb{x}$-component, and zero everywhere else such that
%
\begin{equation*}
  \mb{N}^{T} = \begin{bmatrix} \mb{I} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \end{bmatrix}
\end{equation*}
The right-preconditioned operator $\mb{K}\mb{K}_{B}^{-1}$ only modifies the $\mb{x}$-components of the output vector directly.
Since the Krylov subspace within GMRES is
\begin{equation*}
  \mathcal{K}_{m}(\mb{K}\mb{K}_{B}^{-1}, \mb{r}) = \text{span} \left\{\mb{r}, \mb{K}\mb{K}_{B}^{-1}\mb{r}, \left(\mb{K}\mb{K}_{B}^{-1}\right)^{2} \mb{r}, \ldots, \left(\mb{K}\mb{K}_{B}^{-1}\right)^{m-1} \mb{r} \right\},
\end{equation*}
all vectors in the GMRES algorithm consist of different $\mb{x}$-component values, while all remaining components are scalar multiples of $\mb{r}$.
This property can be used to reduce the memory requirements of GMRES by storing only the $\mb{x}$-components of each vector and a scalar for all remaining components.
Using this approach, the full vector $\hat{\mb{v}}_{i}$ is stored as a pair $(\mb{v}_{i}, \alpha_{i})$ which can be extracted as
%
\begin{equation*}
  \hat{\mb{v}}_{i} = \mb{N}\mb{v}_{i} + \alpha_{i} (\mb{I} - \mb{N}\mb{N}^{T})\mb{r}
\end{equation*}
Using this representation, the dot product of two vectors $\hat{\mb{v}}_{1}$ and $\hat{\mb{v}}_{2}$ stored as $(\mb{v}_{1}, \alpha_{1})$ and $(\mb{v}_{2}, \alpha_{2})$ can be expressed as
%
\begin{equation*}
  \begin{aligned}
    \hat{\mb{v}}_{1}^{T}\hat{\mb{v}}_{2} & =
    \mb{v}_{1}^{T} \mb{N}^{T}\mb{N}\mb{v}_{2} +
    \alpha_{1}\alpha_{2} \mb{r}^{T} \left(\mb{I} - \mb{N} \mb{N}^{T}\right)\mb{r} \\
      &= \mb{v}_{1}^{T} \mb{v}_{2} + \beta_{r} \alpha_{1}\alpha_{2}
  \end{aligned}
\end{equation*}
where $\beta_{r} \triangleq \mb{r}^{T} \left(\mb{I} - \mb{N} \mb{N}^{T}\right)\mb{r}$.

When using a line search method, it is necessary to obtain a descent direction.
This is guaranteed for the methods which employ a quasi-Newton approximation by maintaining a positive-definite Hessian and exactly satisfying the constraint condition $\mb{A}\mb{p}_{x} = -\mb{c}$ (omitting the slack variables).
However, in the exact Newton method a descent direction is not guaranteed.
When the objective is convex, the Hessian is positive semi-definite and a descent direction can be found if the equations are solved to sufficient precision.
Within the present algorithm, the convergence criteria is modified within GMRES to require a sufficiently accurate solution that satisfies a descent criteria.
A penalty parameter $\nu$ for the exact $\ell_{2}$ merit function can be found if either
\begin{equation}
  \label{eqn:descent-function}
  \mb{p}_{x}^{T} \nabla_{x} f < 0,
\end{equation}
or
\begin{equation}
  \label{eqn:descent-constraint}
  \mb{c}^{T}\mb{A} \mb{p}_{x} \le - \gamma ||\mb{c}||_{2}^2,
\end{equation}
for some $0 < \gamma < 1$.
Therefore, if either condition is satisfied we will have a descent direction.

While it is possible to build the full solution at each GMRES iteration, and check the criteria~\eqref{eqn:descent-function} and~\eqref{eqn:descent-constraint}, it is more efficient to modify GMRES to evaluate these criteria indirectly.
GMRES works by building an orthogonal subspace $\mb{V}_{k} \in \mathbb{R}^{n \times k}$ using Arnoldi's method that satisfies the following equation
\begin{equation*}
  \mb{K}\mb{K}_{B}^{-1}\mb{V}_{k} = \mb{V}_{k+1} \bar{\mb{H}}_{k+1}
\end{equation*}
where the approximate solution is $\mb{p} = \mb{K}_{B}^{-1}\mb{V}_{k} \mb{y}_{k}$ where $\mb{y}_{k}$ is obtained from the solution of the least-squares problem
\begin{equation*}
  \mb{y}_{k} = \text{arg} \min_{y} || \bar{\mb{H}}_{k} \mb{y} - \beta \mb{e}_{1} ||_{2}.
\end{equation*}
At each iteration, we compute the action of the matrix $\mb{w} \leftarrow \mb{K}\mb{K}_{B}^{-1}\mb{v}$ by first computing the intermediate vector $\mb{z} = \mb{K}_{B}^{-1} \mb{v}$.
Next, the $\mb{x}$-components of the output vector $\mb{w}$ are obtained by computing
%
\begin{equation*}
  \mb{w}_{x} = \mb{v}_{x} + (\mb{H} - \mb{B})\mb{z}_{x}.
\end{equation*}
Before discarding the intermediate vector $\mb{z}$, we compute the directional derivatives of the objective and the $\ell_{2}$ norm of the constraints, respectively as follows
\begin{equation*}
  a_{k} = \mb{z}_{x}^{T} \nabla f \qquad\qquad b_{k} = \mb{z}_{x}^{T} \mb{A}^{T} \mb{c}.
\end{equation*}
%
Based on these values, the directional derivative of the inexact solution at iteration $k$ of GMRES can be evaluated as
\begin{equation*}
  \begin{aligned}
    \mb{p}_{x}^{T} \nabla f &= \mb{y}_{k}^{T}\mb{a}_{k}, \\
    \mb{p}_{x}^{T} \mb{A}^{T} \mb{c} &= \mb{y}_{k}^{T}\mb{b}_{k}.
  \end{aligned}
\end{equation*}
%
These quantities can be computed inexpensively by evaluating $\mb{y}_{k}$ at every iteration of GMRES, rather than after the final iteration.
If GMRES fails to find an inexact solution that is also a descent direction, we revert back to the quasi-Newton step which is guaranteed to produce a descent direction.

\begin{figure}
  \begin{algorithmic}
    \State{GMRES(m, $\gamma$): Inexactly solve $\mb{K}\mb{p} = \mb{b}$ while ensuring $\mb{p}$ is a descent direction}
    \State{Given the relative stopping tolerance $\epsilon_{r}$ and the descent fraction $\gamma$}
    \State{Evaluate $\beta = ||\mb{b}||_{2}$ and $\beta_{r} = \mb{b}^{T} \left(\mb{I} - \mb{N} \mb{N}^{T}\right)\mb{b}$}
    \State{Set $\mb{v}_{1} = \beta^{-1}\mb{b}$ and $k = 1$}
    %
    \While{$k \le m$}
      \State{Compute $\mb{z} = \mb{K}_{B}^{-1} \mb{v}_{k}$}
      \State{Compute $a_{k} = \mb{z}^{T}\mb{N}^{T}\nabla_{x} f$}
      \State{Compute $b_{k} = \mb{z}^{T}\mb{N}^{T}\mb{A}^{T}\mb{c}$}
      \State{Set $\mb{a}_{k} = (\mb{a}_{k-1}, a_{k})$ and $\mb{b}_{k} = (\mb{b}_{k-1}, b_{k})$}
      \State{Compute $\mb{w} \leftarrow \mb{K}\mb{z}$}
      \State{Compute $\mb{V}_{k+1} \leftarrow MGS(\mb{V}_{k}, \mb{w})$}
      \State{Solve $\mb{y}_{k} = \text{arg} \min_{\mb{y}} || \bar{\mb{H}}_{k} \mb{y} - \beta \mb{e}_{1}||_{2}$}
      \If{$\mb{y}_{k}^{T}\mb{a}_{k} < 0$ or $\mb{y}_{k}^{T}\mb{b}_{k} \le -\gamma ||\mb{c}||_{2}^{2}$}
        \If{$||\bar{\mb{H}}_{k} \mb{y} - \beta \mb{e}_{1}||_{2} < \epsilon_{r} \beta$}
          \State{Successfully found inexact solution satisfying descent direction criteria.}
          \State{\textbf{break}}
        \EndIf
      \EndIf
      \State{$k \leftarrow k + 1$}
    \EndWhile
    \State{Set $\mb{z} = \mb{V}_{k} \mb{y}_{k}$}
    \State{Compute $\mb{p} = \mb{K}_{B}^{-1}\mb{z}$}
    \State{Verify $\mb{p}^{T}\mb{N}\nabla_{x} f < 0$ or $\mb{p}^{T}\mb{N}^{T} \mb{A}^{T}\mb{c} \le -\gamma ||\mb{c}||_{2}^{2}$}
  \end{algorithmic}
  \caption{GMRES with descent direction convergence criteria}
\end{figure}

\section{Trust region algorithm}

ParOpt also implements a trust region method to solve optimization problems. For each trust region iteration, ParOpt constructs a quadratic model subproblem and solves the subproblem using the interior point solver. The general form of the optimization problem is same as the previous one:
%
\begin{equation}
  \label{eqn:opt-problem-tr}
  \begin{aligned}
    \min_{\mb{x}} \qquad & f(\mb{x}) \\
    \text{such that} \qquad
    & \mb{c}(\mb{x}) \ge 0 \\
    & \mb{c}_{w}(\mb{x}) \ge 0 \\
    & \mb{l} \le \mb{x} \le \mb{u}
  \end{aligned}
\end{equation}

Accordingly, the trust region quadratic subproblem with linearized constraints can be formulated as follows, notice that for the rest equations of this section, we drop the subscript $k$ (which indicates the current trust region iteration) to simplify the notation.
%
\begin{equation}
  \label{eqn:quad-problem}
  \begin{aligned}
    \min_{\mb{p}} \qquad & f + \mb{g}^T\mb{p} + \frac{1}{2}\mb{p}^T\mb{B}\mb{p} \\
    \text{such that} \qquad
    & \mb{A}\mb{p} + \mb{c} \ge 0 \\
    & \mb{A}_{w}\mb{p} + \mb{c}_{w} \ge 0 \\
    & \mb{l} \le \mb{x}_k + \mb{p} \le \mb{u} \\
    & ||\mb{p}||_\infty \le \Delta
  \end{aligned}
\end{equation}
%
where $\mb{p} = \mb{x}_{k+1} - \mb{x}_{k}$, which is the update step that we solve for. $f = f(\mb{x}_k)$, $\mb{g}_k = \nabla_x f(\mb{x}_k)$ are the function value and gradient, respectively. $\mb{B} = \nabla_x^2 (f(\mb{x}_k) - \mb{z}^T \mb{c}(\mb{x}_k)-\mb{z}_w^T \mb{c}_w(\mb{x}_k))$ is the approximation of the Hessian of the Lagrangian. $\mb{A}, \mb{c}, \mb{A}_w, \mb{c}_w$ are Jacobian and value for dense constraints and sparse constraints, respectively. $\Delta$ is the trust region radius.

Next, we would like to convert this subproblem into a barrier problem so that it can be solved by ParOpt interior point solver. First, notice that we have the identity
%
\begin{equation}
    \alpha = \max(0, \alpha) - \max(0, -\alpha)
\end{equation}
%
and corresponding relations
%
\begin{equation}
\label{max-relation}
\begin{gathered}
    \max(0, \alpha) \ge 0 \\
    \max(0, -\alpha) \ge 0 \\
    \max(0, \alpha)\cdot \max(0, -\alpha) = 0
\end{gathered}
\end{equation}
%
hold for arbitrary scalar variable $\alpha$. Then for the linearized constraints, we can define slack variables
%
\begin{equation}
    \mb{A}\mb{p} + \mb{c} = \mb{s} - \mb{t}
\end{equation}
%
such that
%
\begin{equation}
\begin{aligned}
    \mb{s} & = \max(\mb{0}, \mb{A}\mb{p} + \mb{c}) \\
    \mb{t} & = \max(\mb{0}, -(\mb{A}\mb{p} + \mb{c}))
\end{aligned}
\end{equation}
%
where the $\max$ function is applied to vectors in the component-wise fashion. Due to the characteristics of the $\max$ function shown in equation (\ref{max-relation}), we will have additional conditions for the slack variable $\mb{s}$ and $\mb{t}$:
%
\begin{equation}
\label{eqn:S}
    \begin{gathered}
    \mb{s} \ge \mb{0} \\
    \mb{t} \ge \mb{0} \\
    \mb{S} \mb{t} = \mb{0}
    \end{gathered}
\end{equation}
%
where $\mb{S}$ is the square matrix diagonalized from vector $\mb{s}$, note that this convention is used multiple times in the future.

Go back to the subproblem (\ref{eqn:quad-problem}), now we can move the constraints into the objective function by using $l_1$ penalization and slack variables that we just defined:
%
\begin{equation}
  \label{eqn:quad-problem-penalty}
  \begin{aligned}
    \min_{\mb{p}, \mb{s}, \mb{t}, \mb{s}_w} \qquad & f + \mb{g}^T\mb{p} + \frac{1}{2}\mb{p}^T\mb{B}\mb{p} + \gamma_t^T\mb{t} + \gamma_s^T\mb{s}\\
    \text{such that} \qquad
    & \mb{A}\mb{p} + \mb{c} = \mb{s} - \mb{t} \\
    & \mb{A}_{w}\mb{p} + \mb{c}_{w} = \mb{s}_w \\
    & \mb{l}_0 \le \mb{p} \le \mb{u}_0 \\
    & \mb{s}_w \ge \mb{0} \\
    & \mb{s} \ge \mb{0} \\
    & \mb{t} \ge \mb{0} \\
    & \mb{S} \mb{t} = \mb{0}
  \end{aligned}
\end{equation}
%
where $\mb{l}_0 = \max(\mb{l} - \mb{x}_k, -\Delta)$, $\mb{u}_0 = \min(\mb{u} - \mb{x}_k, \Delta)$. Here notice that we use both $\mb{s}$ and $\mb{t}$ as penalty terms, because this allows us to apply not only inequality constraints but also equality constraints for $\mb{c}$. If we want $\mb{c}$ to be equality constraints, then set $\gamma_t > 0, \gamma_s > 0$, and $\mb{s} + \mb{t} = |\mb{A}\mb{p} + \mb{c}|$ is the constraint violation to be added as penalty. Otherwise, if we want to keep $\mb{c}$ as inequality constraints, then set $\gamma_t > 0, \gamma_s = 0$, and only $\mb{t}$ will be added as penalty.

Finally, by moving inequality constraints in equation (\ref{eqn:quad-problem-penalty}) into the objective function using log barrier function, the barrier problem can be formulated:
%
\begin{equation}
  \label{eqn:quad-problem-barrier}
  \begin{aligned}
    \min_{\mb{p}, \mb{s}, \mb{t}, \mb{s}_w} \qquad & f + \mb{g}^T\mb{p} + \frac{1}{2}\mb{p}^T\mb{B}\mb{p} + \gamma_t^T\mb{t} + \gamma_s^T\mb{s} - \mu \left[ \log \mb{s} + \log \mb{t} + \log \mb{s}_{w} + \log (\mb{p} - \mb{l}_0) + \log (\mb{u}_0 - \mb{p})  \right] \\
    \text{such that} \qquad
    & \mb{A}\mb{p} + \mb{c} = \mb{s} - \mb{t} \\
    & \mb{A}_{w}\mb{p} + \mb{c}_{w} = \mb{s}_w \\
    & \mb{S} \mb{t} = \mb{0}
  \end{aligned}
\end{equation}

Lastly, it can be shown that this problem can be further simplified by dropping the last constraint (see appendix \ref{section:proof} for proof):
%
\begin{equation}
  \label{eqn:quad-problem-barrier-2}
  \begin{aligned}
    \min_{\mb{p}, \mb{s}, \mb{t}, \mb{s}_w} \qquad & f + \mb{g}^T\mb{p} + \frac{1}{2}\mb{p}^T\mb{B}\mb{p} + \gamma_t^T\mb{t} + \gamma_s^T\mb{s} - \mu \left[ \log \mb{s} + \log \mb{t} + \log \mb{s}_{w} + \log (\mb{p} - \mb{l}_0) + \log (\mb{u}_0 - \mb{p})  \right] \\
    \text{such that} \qquad
    & \mb{A}\mb{p} + \mb{c} = \mb{s} - \mb{t} \\
    & \mb{A}_{w}\mb{p} + \mb{c}_{w} = \mb{s}_w
  \end{aligned}
\end{equation}

This is the actual form of problem that ParOpt interior point optimizer will solve.

In consistent with $l_1$ penalization, the $l_1$ merit function
%
\begin{equation}
    \label{eqn:l1-merit-function}
    \phi(\mb{x}) = f(\mb{x}) + \gamma_s^T \max(\mb{0}, \mb{c}(\mb{x})) + \gamma_t^T \max(\mb{0}, -\mb{c}(\mb{x}))
\end{equation}
%
and model merit function
%
\begin{equation}
    \label{eqn:model-merit-function}
    q(\mb{p}) = f + \mb{g}^T\mb{p} + \frac{1}{2}\mb{p}^T\mb{B}\mb{p} + \gamma_s^T \max(\mb{0}, \mb{A}\mb{p} + \mb{c}) + \gamma_t^T \max(\mb{0}, -(\mb{A}\mb{p} + \mb{c}))
\end{equation}
%
are used to decide if the solution $\mb{p}$ can be accepted or rejected. The step acceptance is determined by computing the ratio:
\begin{equation}
    \label{eqn:rho_k}
    \rho = \frac{\mbox{actual reduction}}{\mbox{model reduction}} \\
    = \frac{\phi(\mb{x}) - \phi(\mb{x} + \mb{p})}{q(\mb{0}) - q(\mb{p})}
\end{equation}

\subsection{Update penalty parameter}
In order to obtain good performance, the penalty parameter $\gamma$ must be chosen carefully at each iteration. In ParOpt, the steering method is used to update $\gamma$ in an adaptive way. The idea of this steering method is to choose the penalty parameter $\gamma$ at each iteration such that it is small enough to make sure we are decreasing the objective function, and large enough to cause the step to make sufficient progress in the linearized feasibility. It is a strategy that ensures balanced progress toward feasibility and optimality.

Define model constraint violation by:
\begin{equation}
    m_k(\mb{p}) = \gamma_s^T \max(\mb{0}, \mb{A}\mb{p} + \mb{c}) + \gamma_t^T \max(\mb{0}, -(\mb{A}\mb{p} + \mb{c}))
\end{equation}

Then, the objective of $l_1$ penalty problem (\ref{eqn:quad-problem-penalty}) can be written as
\begin{equation}
    q_k(\mb{p}) = f + \mb{g}^T\mb{p} + \frac{1}{2}\mb{p}^T\mb{B}\mb{p} + m_k(\mb{p})
\end{equation}

In every trust region iteration, besides the original $l_1$ penalty problem (\ref{eqn:quad-problem-penalty}), the following linear programming problem is also solved:
\begin{equation}
  \label{eqn:steering-linear}
  \begin{aligned}
    \min_{\mb{p}} \qquad & m_k(\mb{p})\\
    \text{such that} \qquad
    & \mb{l} \le \mb{x}_k + \mb{p} \le \mb{u} \\
    & ||\mb{p}||_\infty \le \Delta_k
  \end{aligned}
\end{equation}

In practice this is done by setting the penalty parameter $\gamma$ in the original problem (\ref{eqn:quad-problem-penalty}) to a very large value (e.g. $\gamma = 10^6$) and recall the optimization routine to approximately solve the linear problem (\ref{eqn:steering-linear}). The solutions to the original penalty problem and linear programming problem are denoted as $\mb{p}_k$ and $\mb{p}_\infty$, respectively. Then $m_k(0) - m_k(\mb{p}_\infty)$ is the best constraint infeasibility reduction that we can achieve within the trust region radius regardless the objective function, and $m_k(0) - m_k(\mb{p}_k)$ is the model infeasibility reduction at $\mb{p}_k$.

Next, we may update $\gamma_{k+1}$ based on these results. The general ideas are: if $m_k(0) \approx 0$, meaning that we've already been in the feasible region, then we might want to decrease $\gamma_{k+1}$ such that we can decrease the objective function itself faster without worrying about constraint infeasibility because we are using interior point method. If $m_k(0) > 0$, and model infeasibility reduction is smaller than best reduction, meaning that the constraint penalization is not significant enough and we want to move faster towards feasible region. Thus we might want to increase $\gamma_{k+1}$.

Finally, the complete trust region S$l_1$QP algorithm with adaptive $\gamma$ update is presented in figure (\ref{fig:trust-region-alg}).

\begin{figure}
  \begin{algorithmic}
  \State{Choose stop criteria $\epsilon_{infeas}, \epsilon_{KKTerror} > 0$}
  \State{Choose trust region acceptance ratio $\eta \in (0, 1)$, trust
  region radii $\Delta_{max}, \Delta_{min}$}
  \State{Choose penalty parameters $\gamma_{max}, \gamma_{min}$}
  \State{Choose initial $\mb{x}_1, \Delta_1, \gamma_1$}
  \For{$k=1,2,...,\mbox{max\_iter}$}
  \State{Evaluate $f_k, \mb{c}_k, \mb{g}_k, \mb{B}_k, \mb{A}_k, \mb{z}_k$}
  \If{$\mbox{infeasibility} < \epsilon_{infeas}$ and $\mbox{KKTerror} < \epsilon_{KKTerror}$}
  \State{\textbf{break}}
  \EndIf
  \State{Solve the trust region subproblem to get update $\mb{p}_k$}
  \State{Compute the ratio $\rho_k$ (equation \ref{eqn:rho_k})}
  \If{$\rho_k \ge \eta$ or $\Delta_k = \Delta_{min}$}
  \State{Accept the trust region update: set $\mb{x}_{k+1} = \mb{x}_k + \mb{p}_k$}
  \Else{}
  \State{Reject the trust region update: set $\mb{x}_{k+1} = \mb{x}_k$}
  \EndIf
  \If{$\rho_k < 0.25$}
  \State{Set $\Delta_{k+1} = \max(0.25\Delta_k,\Delta_{min})$}
  \ElsIf{$\rho_k > 0.75$}
  \State{Set $\Delta_{k+1} = \min(1.5\Delta_k, \Delta_{max})$}
  \Else{}
  \State{$\Delta_{k+1} = \Delta_k$}
  \EndIf
  \State{Solve the linear programming problem (\ref{eqn:steering-linear}) to get $\mb{p}_{\infty}$}
  \State{Compute $m_k(0) - m_k(\mb{p}_k)$ and $m_k(0) - m_k(\mb{p}_\infty)$}
  \For{$i = 1, 2, ...$}
  \If{$m_k(0)_i < \epsilon_{infeas}$ and $0.5{\gamma_k}_i \ge {\mb{z}_k}_i > \epsilon_{infeas}$}
  \State{Reduce penalty parameter: ${\gamma_{(k+1)}}_i = 0.5({\gamma_k}_i + {\mb{z}_k}_i) + \gamma_{min}$}
  \ElsIf{$m_k(0)_i > \epsilon_{infeas}$ and $0.995(m_k(0) - m_k(\mb{p}_\infty))_i > (m_k(0) - m_k(\mb{p}_k))_i$}
  \State{Increase penalty parameter: ${\gamma_{(k+1)}}_i = \min({1.5\gamma_k}_i, \gamma_{max})$}
  \Else{}
  \State{${\gamma_{(k+1)}}_i = {\gamma_k}_i$}
  \EndIf
  \EndFor
  \EndFor
  \end{algorithmic}
  \caption{Trust region S$l_1$QP algorithm with adaptive $\gamma$ update}
  \label{fig:trust-region-alg}
\end{figure}


\section{Implementation details}

The two main classes need by users of ParOpt are \texttt{ParOptProblem} and \texttt{ParOpt}.
These classes are both accessible using Python through a Cython wrapper.
The \texttt{ParOptProblem} class is an abstract base class that is designed to implement the functions needed to solve an optimization problem.
It is responsible for evaluating functions and constraints as well as allocating parallel design and sparse constraint vectors.
The python-level implementation of this class uses a default vector implementation where the components of the vector are distributed across all processors in the communicator provided to ParOpt.

The \texttt{ParOpt} class itself is responsible for optimizing the problem.
All options are set through public member-access functions that can be called from the python or C++ interfaces.
These functions will be described below.

\subsection{ParOptProblem class methods}

The constructor for the \texttt{ParOptProblem} class takes the following form:
%
\begin{verbatim}
ParOptProblem( MPI_Comm _comm,
               int _nvars, int _ncon,
               int _nwcon, int _nwblock );
\end{verbatim}

The arguments to this constructor are:
\begin{itemize}
\item \texttt{comm} is the MPI communicator for the problem.
This communicator will be passed to the corresponding \texttt{ParOpt} optimizer class.

\item \texttt{nvars}: The local number of design variables that are owned by this processor.

\item \texttt{ncon}: The global number of dense constraints.

\item \texttt{nwcon}: The local number of sparse constraints that are owned by this processor

\item \texttt{nwblock}: The size of the block in block-diagonal matrix formed from the weighting constraints $\mb{A}_{w}^{T}\mb{A}_{w}$. This size must be the same on all processors.
\end{itemize}

The following functions are used to specify the structure of the design problem:
\begin{enumerate}
\item \texttt{int isDenseInequality()}: Are the dense constraints inequality or equality constraints?
\item \texttt{int isSparseInequality()}: Are the sparse constraints inequality or equality constraints?
\item \texttt{int useLowerBounds()}: Should the optimizer use the lower bound constraints?
\item \texttt{int useUpperBounds()}: Should the optimizer use the upper bound constraints?
\end{enumerate}

\subsubsection{ParOptProblem evaluation functions}

The following evaluation member functions must be defined. Their arguments are omitted here, but can be found
in the file \texttt{src/ParOptProblem.h}.
\begin{enumerate}
\item \texttt{getVarsAndBounds}: Get the design variables at the starting point as well as the lower and upper bounds for the variables. This is called when \texttt{ParOpt} is initialized.

\item \texttt{int evalObjCon}: Given the design variable values, evaluate the objective and dense constraint functions. This returns a fail flag. When the fail flag is non-zero, the function has failed.

\item \texttt{int evalObjConGradient}: Given the design variable values, evaluate the objective and constraint gradients. The objective is a single \texttt{ParOptVec} instance. The dense constraint Jacobian is an array of \texttt{ParOptVec} instances. This function also returns a fail flag.

\item \texttt{int evalHvecProduct}: Given the design variable values, the multipliers for the dense and sparse constraints, and a direction in the design space, compute the Hessian-vector product. Note that this function combines computations using the sparse and dense constraints. This function also returns a fail flag.
\end{enumerate}

\subsubsection{ParOptProblem sparse constraint functions}

The following functions are used exclusively for the sparse constraints
%
\begin{enumerate}
\item \texttt{evalSparseCon}: Given the design variables, evaluate the sparse constraints.

\item \texttt{addSparseJacobian} Given a scalar, the design variables, and an input direction vector the size of the number of design variables, compute the scaled Jacobian-vector product of the sparse constraints.

\item \texttt{addSparseJacobianTranspose} Given a scalar, the design variables, and an input direction vector the same size as the number of sparse constraints, compute the scaled transpose Jacobian-vector product of the sparse constraints. This function is required for computing the product $\mb{A}_{w}^{T}\mb{z}_{w}$.

\item \texttt{addSparseInnerProduct} Given a scalar, the design variable vector, and the diagonal of a square matrix the size of the number of sparse constraints, compute add the product, $\mb{D} \leftarrow \mb{D} + \alpha \mb{A}_{w} \mb{S}\mb{A}_{w}^{T}$.
\end{enumerate}


ParOpt implements a default vector class.
It is often convenient or necessary to override this class for certain design problems.
ParOptProblem can override the default implementation if the following two functions are provided:
\begin{enumerate}
\item \texttt{ParOptVec *createDesignVec()}: Create a vector with the right type and shape to store design variables/objective gradients.
\item \texttt{ParOptVec *createConstraintVec()}: Create a vector with the right type and shape to store a sparse constraint vector
\end{enumerate}

The function \texttt{void writeOutput( int iter, ParOptVec *x )} can be overridden to write out design-dependent data at a specified frequency.

\subsection{ParOptOptimizer interface class}

\texttt{ParOptOptimizer} is a generic interface to all ParOpt optimization algorithms.
This class should be used to initialize and run ParOpt optimizers.
The process to initialize, optimize and retrieve results is as follows:

\begin{enumerate}
\item Retrieve the default options for all ParOpt optimizers by calling the static member function
\texttt{static void addDefaultOptions( ParOptOptions *options );}.

\item Instantiate the \texttt{ParOptOptimizer} class with a problem instance by calling \texttt{ParOptOptimizer( ParOptProblem *problem, ParOptOptions *options );}

\item Perform the optimization by calling the member function \texttt{void optimize();}

\item Get the optimized point and multiplier values by calling the member function
\begin{verbatim}
void getOptimizedPoint( ParOptVec **x, ParOptScalar **z,
                        ParOptVec **zw, ParOptVec **zl, ParOptVec **zu );
\end{verbatim}
\end{enumerate}

\subsection{Default values of ParOptOptions}

The full list of ParOpt options can be obtained by using the python interface and entering the following command:
\begin{verbatim}
python -c 'from paropt import ParOpt; ParOpt.printOptionSummary()'
\end{verbatim}
%
Note that options with a prefix \texttt{tr\_} apply generally to the trust region method, while options with \texttt{mma\_} apply generally to the method of moving asymptotes.
Options without either of these prefixes apply to the interior point method.
The above command gives the following output:

{\footnotesize
\begin{verbatim}
Absolute stopping criterion
abs_res_tol                              1e-06
Range of values: lower limit 0  upper limit 1e+20

Absolute stopping norm on the step size
abs_step_tol                             0
Range of values: lower limit 0  upper limit 1e+20

The type of optimization algorithm
algorithm                                tr
Range of values:                         ip
                                         tr
                                         mma

The Armijo constant for the line search
armijo_constant                          1e-05
Range of values: lower limit 0  upper limit 1

The type of barrier update strategy to use
barrier_strategy                         monotone
Range of values:                         monotone
                                         mehrotra
                                         mehrotra_predictor_corrector
                                         complementarity_fraction

The absolute precision of the design variables
design_precision                         1e-14
Range of values: lower limit 0  upper limit 1

Exponent in the Eisenstat-Walker INK forcing equation
eisenstat_walker_alpha                   1.5
Range of values: lower limit 0  upper limit 2

Multiplier in the Eisenstat-Walker INK forcing equation
eisenstat_walker_gamma                   1
Range of values: lower limit 0  upper limit 1

The absolute precision of the function and constraints
function_precision                       1e-10
Range of values: lower limit 0  upper limit 1

The absolute GMRES tolerance (almost never relevant)
gmres_atol                               1e-30
Range of values: lower limit 0  upper limit 1

The subspace size for GMRES
gmres_subspace_size                      0
Range of values: lower limit 0  upper limit 1000

Step length used to check the gradient
gradient_check_step_length               1e-06
Range of values: lower limit 0  upper limit 1

Print to screen the output of the gradient check at this frequency during an optimization
gradient_verification_frequency          -1
Range of values: lower limit -1000000  upper limit 1000000

Do a hard reset of the Hessian at this specified major iteration frequency
hessian_reset_freq                       1000000
Range of values: lower limit 1  upper limit 1000000

The initial value of the barrier parameter
init_barrier_param                       0.1
Range of values: lower limit 0  upper limit 1e+20

Initial value of the line search penalty parameter
init_rho_penalty_search                  0
Range of values: lower limit 0  upper limit 1e+20

Checkpoint file for the interior point method
ip_checkpoint_file                       None

Maximum bound value at which bound constraints are omitted
max_bound_value                          1e+20
Range of values: lower limit 0  upper limit 1e+300

The maximum relative tolerance used for GMRES, above this the quasi-Newton approximation is used
max_gmres_rtol                           0.1
Range of values: lower limit 0  upper limit 1

Maximum number of line search iterations
max_line_iters                           10
Range of values: lower limit 1  upper limit 100

The maximum number of major iterations before quiting
max_major_iters                          5000
Range of values: lower limit 0  upper limit 1000000

Minimum fraction to the boundary rule < 1
min_fraction_to_boundary                 0.95
Range of values: lower limit 0  upper limit 1

Minimum value of the line search penalty parameter
min_rho_penalty_search                   0
Range of values: lower limit 0  upper limit 1e+20

Contraction factor applied to the asymptotes
mma_asymptote_contract                   0.7
Range of values: lower limit 0  upper limit 1

Expansion factor applied to the asymptotes
mma_asymptote_relax                      1.2
Range of values: lower limit 1  upper limit 1e+20

Relaxation bound for computing the error in the KKT conditions
mma_bound_relax                          0
Range of values: lower limit 0  upper limit 1e+20

Regularization term applied in the MMA approximation
mma_delta_regularization                 1e-05
Range of values: lower limit 0  upper limit 1e+20

Regularization term applied in the MMA approximation
mma_eps_regularization                   0.001
Range of values: lower limit 0  upper limit 1e+20

Infeasibility tolerance
mma_infeas_tol                           1e-05
Range of values: lower limit 0  upper limit 1e+20

Initial aymptote offset from the variable bounds
mma_init_asymptote_offset                0.25
Range of values: lower limit 0  upper limit 1

l1 tolerance for the optimality tolerance
mma_l1_tol                               1e-06
Range of values: lower limit 0  upper limit 1e+20

l-infinity tolerance for the optimality tolerance
mma_linfty_tol                           1e-06
Range of values: lower limit 0  upper limit 1e+20

Maximum asymptote offset from the variable bounds
mma_max_asymptote_offset                 10
Range of values: lower limit 0  upper limit 1e+20

Maximum number of iterations
mma_max_iterations                       200
Range of values: lower limit 0  upper limit 1000000

Minimum asymptote offset from the variable bounds
mma_min_asymptote_offset                 0.01
Range of values: lower limit 0  upper limit 1e+20

Ouput file name for MMA
mma_output_file                          paropt.mma

If false, linearized the constraints
mma_use_constraint_linearization         True

Factor applied to the barrier update < 1
monotone_barrier_fraction                0.25
Range of values: lower limit 0  upper limit 1

Exponent for barrier parameter update > 1
monotone_barrier_power                   1.1
Range of values: lower limit 1  upper limit 10

Switch to the Newton-Krylov method at this residual tolerance
nk_switch_tol                            0.001
Range of values: lower limit 0  upper limit 1e+20

The type of norm to use in all computations
norm_type                                infinity
Range of values:                         infinity
                                         l1
                                         l2

Output file name
output_file                              paropt.out

Output level indicating how verbose the output should be
output_level                             0
Range of values: lower limit 0  upper limit 1000000

Fraction of infeasibility used to enforce a descent direction
penalty_descent_fraction                 0.3
Range of values: lower limit 1e-06  upper limit 1

l1 penalty parameter applied to slack variables
penalty_gamma                            1000
Range of values: lower limit 0  upper limit 1e+20

The problem name
problem_name                             None

Scalar added to the diagonal of the quasi-Newton approximation > 0
qn_sigma                                 0
Range of values: lower limit 0  upper limit 1e+20

The maximum dimension of the quasi-Newton approximation
qn_subspace_size                         10
Range of values: lower limit 0  upper limit 1000

The the of quasi-Newton approximation to use
qn_type                                  bfgs
Range of values:                         bfgs
                                         sr1
                                         none

The type of BFGS update to apply when the curvature condition fails
qn_update_type                           skip_negative_curvature
Range of values:                         skip_negative_curvature
                                         damped_update

Relative factor applied to barrier parameter for bound constraints
rel_bound_barrier                        1
Range of values: lower limit 0  upper limit 1e+20

Relative function value stopping criterion
rel_func_tol                             0
Range of values: lower limit 0  upper limit 1e+20

Discard the quasi-Newton approximation (but not necessarily the exact Hessian)
sequential_linear_method                 False

Minimum multiplier for the affine step initialization strategy
start_affine_multiplier_min              1
Range of values: lower limit 0  upper limit 1e+20

Initialize the Lagrange multiplier estimates and slack variables
starting_point_strategy                  affine_step
Range of values:                         least_squares_multipliers
                                         affine_step
                                         no_start_strategy

The type of constraint to use for the adaptive penalty subproblem
tr_adaptive_constraint                   linear_constraint
Range of values:                         linear_constraint
                                         subproblem_constraint

Adaptive penalty parameter update
tr_adaptive_gamma_update                 True

The type of objective to use for the adaptive penalty subproblem
tr_adaptive_objective                    linear_objective
Range of values:                         constant_objective
                                         linear_objective
                                         subproblem_objective

Upper and lower bound relaxing parameter
tr_bound_relax                           0.0001
Range of values: lower limit 0  upper limit 1e+20

Trust region trial step acceptance ratio
tr_eta                                   0.25
Range of values: lower limit 0  upper limit 1

Infeasibility tolerance
tr_infeas_tol                            1e-05
Range of values: lower limit 0  upper limit 1e+20

The initial trust region radius
tr_init_size                             0.1
Range of values: lower limit 0  upper limit 1e+20

l1 tolerance for the optimality tolerance
tr_l1_tol                                1e-06
Range of values: lower limit 0  upper limit 1e+20

l-infinity tolerance for the optimality tolerance
tr_linfty_tol                            1e-06
Range of values: lower limit 0  upper limit 1e+20

Maximum number of trust region iterations
tr_max_iterations                        200
Range of values: lower limit 0  upper limit 1000000

The maximum trust region radius
tr_max_size                              1
Range of values: lower limit 0  upper limit 1e+20

The minimum trust region radius
tr_min_size                              0.001
Range of values: lower limit 0  upper limit 1e+20

Trust region output file
tr_output_file                           paropt.tr

Maximum value for the penalty parameter
tr_penalty_gamma_max                     10000
Range of values: lower limit 0  upper limit 1e+20

Minimum value for the penalty parameter
tr_penalty_gamma_min                     0
Range of values: lower limit 0  upper limit 1e+20

The barrier update strategy to use for the steering method subproblem
tr_steering_barrier_strategy             mehrotra_predictor_corrector
Range of values:                         monotone
                                         mehrotra
                                         mehrotra_predictor_corrector
                                         complementarity_fraction
                                         default

The barrier update strategy to use for the steering method subproblem
tr_steering_starting_point_strategy      affine_step
Range of values:                         least_squares_multipliers
                                         affine_step
                                         no_start_strategy
                                         default

Write output frequency
tr_write_output_frequency                10
Range of values: lower limit 0  upper limit 1000000

Perform a back-tracking line search
use_backtracking_alpha                   False

Use or do not use the diagonal Hessian computation
use_diag_hessian                         False

Use or do not use Hessian-vector products
use_hvec_product                         False

Perform or skip the line search
use_line_search                          True

Use or do not use the quasi-Newton method as a preconditioner
use_qn_gmres_precon                      True

Update the quasi-Newton approximation at each iteration
use_quasi_newton_update                  True

Write out the solution file and checkpoint file at this frequency
write_output_frequency                   10
Range of values: lower limit 0  upper limit 1000000
\end{verbatim}}


\bibliographystyle{abbrvnat}
\bibliography{refs}

\appendix
\section{Equivalency of the two barrier problems: proof}
\label{section:proof}
We would like to show that for the following two optimization problems, we can replace the first one by second one.
%
\begin{equation}
  \label{eqn:barrier-1}
  \begin{aligned}
    \min_{\mb{x}, \mb{s}, \mb{t}} \qquad & f(\mb{x}) + \gamma \mb{e}^T \mb{t} - \mu [\log \mb{s} + \log \mb{t} ] \\
    \text{such that} \qquad
    & \mb{c}(\mb{x}) = \mb{s} - \mb{t} \\
    & \mb{S} \mb{t} = 0
  \end{aligned}
\end{equation}
%
\begin{equation}
  \label{eqn:barrier-2}
  \begin{aligned}
    \min_{\mb{x}, \mb{s}, \mb{t}} \qquad & f(\mb{x}) + \gamma \mb{e}^T \mb{t} - \mu [\log \mb{s} + \log \mb{t} ] \\
    \text{such that} \qquad
    & \mb{c}(\mb{x}) = \mb{s} - \mb{t}
  \end{aligned}
\end{equation}

In fact, problem (\ref{eqn:barrier-1}) and problem (\ref{eqn:barrier-2}) have the equivalent KKT systems. For problem (\ref{eqn:barrier-1}), define the Lagrangian:
%
\begin{equation}
\mathcal{L}_1(\mb{x}, \mb{s}, \mb{t}; \mb{y}, \mb{z}) = f(\mb{x}) + \gamma \mb{e}^T \mb{t} - \mu [\log \mb{s} + \log \mb{t} ] - \mb{y}^T [\mb{c}(\mb{x}) - \mb{s} + \mb{t}] - \mb{z}^T \mb{S} \mb{t}
\end{equation}

Then the KKT system is:
%
\begin{equation}
  \label{eqn:KKT-1}
  \begin{aligned}
    \nabla_\mb{x}\mathcal{L}_1 &= \nabla f(\mb{x}) - \mb{A}^T(\mb{x}) \mb{y} &= 0\\
    \nabla_\mb{s}\mathcal{L}_1 &= - \mu \mb{S}^{-1}\mb{e} + \mb{y} - \mb{Z} \mb{t} &=  0\\
    \nabla_\mb{t}\mathcal{L}_1 &= \gamma \mb{e} - \mu \mb{T}^{-1} \mb{e} - \mb{y} - \mb{Z} \mb{s} &= 0 \\
    \nabla_\mb{y}\mathcal{L}_1 &= \mb{c}(\mb{x}) - \mb{s} + \mb{t} &= 0\\
    \nabla_\mb{z}^T\mathcal{L}_1 &= \mb{S} \mb{t} &= 0
  \end{aligned}
\end{equation}

Note that here diagonal matrices $\mb{T}$ and $\mb{Z}$ are defined in the same way as in (\ref{eqn:S}). For the second and third equation, multiply through by $\mb{S}$ and $\mb{T}$, respectively, also notice that since $\mb{S}, \mb{T}$ and $\mb{Z}$ are diagonal matrices, we have $\mb{S} \mb{Z} \mb{t}= \mb{Z} \mb{S} \mb{t}, \mb{T}\mb{Z} \mb{s} = \mb{Z}\mb{S}\mb{t}$. Then we can eliminate the last terms in these two equations, because we have $\mb{S}\mb{t}=\mb{0}$. Finally, multiply through by $\mb{S}^{-1}$ and $\mb{T}^{-1}$, respectively, then the KKT system (\ref{eqn:KKT-1}) becomes:
%
\begin{equation}
  \label{eqn:KKT-2}
  \begin{aligned}
    \nabla f(\mb{x}) - \mb{A}^T(\mb{x}) \mb{y} &= 0\\
    - \mu \mb{S}^{-1} \mb{e} + \mb{y}  &=  0\\
    \gamma \mb{e} - \mu \mb{T}^{-1} \mb{e} - \mb{y} &= 0 \\
    \mb{c}(\mb{x}) - \mb{s} + \mb{t} &= 0\\
  \end{aligned}
\end{equation}

And this turns out to be the KKT system for problem (\ref{eqn:barrier-2}), which means that we can get the solution to the problem (\ref{eqn:barrier-1}) by solving the problem (\ref{eqn:barrier-2}), thus these two optimization problems are equivalent in this sense. Also notice that from (\ref{eqn:KKT-1}) to (\ref{eqn:KKT-2}) we exclude the last equation $\mb{S}\mb{t} = \mb{0}$, this is because we have removed the multiplier $\mb{z}$ from the problem, then the last equation of (\ref{eqn:KKT-1}) is redundant for (\ref{eqn:KKT-2}).

\end{document}
