\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{mathptmx}
\usepackage[colorlinks, citecolor=black, linkcolor=black, %
filecolor=black, urlcolor=blue]{hyperref}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algpseudocode}

\newcommand{\mb}{\mathbf}
\newcommand{\p}{\partial}
\newcommand{\mbs}{\boldsymbol}
\newcommand{\f}{\frac}

\title{ParOpt: A parallel interior-point optimizer}
\author{Graeme J. Kennedy}
\date{}

\begin{document}

\maketitle

\section{Introduction}

ParOpt is a parallel gradient-based optimization library implemented in C++ and is intended for solving large-scale constrained optimization problems.
ParOpt can be applied to general purpose optimization problems, but is often used for large-scale topology optimization. 
The constraints in ParOpt fall into one of two categories: (1) constraints with full dependence on the design vector such that the constraint Jacobian is fully populated, or (2) constraints that have a specific sparse structure, described below, that enables them to be grouped independently.
All operations in ParOpt use distributed design vectors and almost all computations are performed in parallel, with a small number of factorization operations performed on small dense matrices in serial.
ParOpt can optionally use information from Hessian-vector products to accelerate convergence.
Within the Hessian-vector product mode, inexact solutions of the KKT system are used where the tolerances are determined using the Eisenstat--Walker forcing terms.

ParOpt utilizes both a C++ and a python-level interface.
The python-level interface is generated using cython.
Call-backs from C++ to python are implemented using direct memory access into numpy arrays.
Some care must be exercised when setting or reading values from arrays passed to python-level functions so as not to inadvertently set gradient or Jacobian values into a copied vector.
Furthermore, the design variable vector passed during callbacks is used by ParOpt, so modification of design vector will produce undesirable results.
ParOpt uses an abstract problem interface with an abstract vector class that can be implemented by application-specific methods.
This enables the use of externally-defined vectors, as long as basic vector-vector and vector-scalar operations are defined.
A default ParOptVec class is implemented to provide generic functionality.

The following document is divided into two sections: (1) a high-level description of the algorithms implemented in ParOpt, and (2) a detailed description of the implementation of ParOpt.

\section{Algorithms}

ParOpt uses an interior-point method to solve optimization problems formulated as follows:
\begin{equation}
  \label{eqn:opt-problem}
  \begin{aligned}
    \min_{\mb{x}} \qquad & f(\mb{x}) \\
    \text{such that} \qquad 
    & \mb{c}(\mb{x}) \ge 0 \\
    & \mb{c}_{w}(\mb{x}) \ge 0 \\
    & \mb{l} \le \mb{x} \le \mb{u}
  \end{aligned}
\end{equation}
Here, $\mb{c}(\mb{x})$ are the dense constraints, and $\mb{c}_{w}(\mb{x})$ are the sparse constraints. 
The sparse constraint Jacobian, $\mb{A}_{w} = \nabla_{x} \mb{c}_{w}(\mb{x})$ must have a structure such that the matrix
\begin{equation*}
  \mb{D} = \mb{A}_{w}\mb{S}\mb{A}_{w}^{T}
\end{equation*}
is a block-diagonal matrix whenever $\mb{S}$ is a diagonal matrix. 
This structure arises in many topology and multimaterial optimization problems that employ weighting constraints for each element within the problem. 

An interior point algorithm approximately solves a sequence of barrier problems that are designed to approach the true constrained minimizer in the limit.
The barrier problem is formed by adding inequality constraints to the objective through a log barrier penalty function. 
This barrier function is designed to keep the iterates strictly in the interior of the feasible region. 
The barrier problem corresponding to~\eqref{eqn:opt-problem} is the following
\begin{equation}
  \label{eqn:barrier-problem}
  \begin{aligned}
    \min_{\mb{x}, \mb{s}, \mb{t}, \mb{s}_{w}} \qquad &  \varphi(\mb{x}, \mb{s}, \mb{t}, \mb{s}_{w}; \mu) = f(\mb{x}) + \gamma \mb{e}^{T}\mb{t} - \mu \left[ \log \mb{s} + \log \mb{t} + \log \mb{s}_{w} + \log (\mb{x} - \mb{l}) + \log (\mb{u} - \mb{x})  \right] \\
    \text{such that} \qquad & \mb{c}(\mb{x}) = \mb{s} - \mb{t} \\
    & \mb{c}_{w}(\mb{x}) = \mb{s}_{w} \\
  \end{aligned}
\end{equation}
where $\mb{s}$, $\mb{t}$ and $\mb{s}_{w}$ are slack variables associated with the dense and sparse constraints, respectively. 
The function $\log$ is the component-wise sum of the logarithms of the vector components, i.e. $\log \mb{s} = \sum_{i} \ln s_{i}$.
As the barrier parameter, $\mu$, decreases, the minimizer of the barrier problem~\eqref{eqn:barrier-problem} approaches the KKT solution.

The barrier problem~\eqref{eqn:barrier-problem} is related to a set of perturbed KKT conditions for the optimization problem~\eqref{eqn:opt-problem}.
Introducing dual variables for the dense constraints $\mb{z}$, the sparse constraints $\mb{z}_{w}$, and the lower and upper bounds, $\mb{z}_{l}$, and $\mb{z}_{u}$, the perturbed KKT conditions can be written as follows:
%
\begin{equation}
  \label{eqn:perturbed-kkt}
  \begin{aligned}
    \mb{r}_{x} \triangleq    & \mb{g} - \mb{A}^{T}\mb{z} - \mb{A}_{w} \mb{z}_{w} - \mb{z}_{l} + \mb{z}_{u} = 0 \\
    \mb{r}_{t} \triangleq    & \gamma \mb{e} - \mb{z} - \mb{z}_{t} = 0 \\
    \mb{r}_{z} \triangleq    & \mb{c} - \mb{s} + \mb{t} = 0 \\
    \mb{r}_{z_{w}} \triangleq & \mb{c}_{w} - \mb{s}_{w} = 0 \\
    \mb{r}_{s} \triangleq    & \mb{S} \mb{z} - \mu \mb{e} = 0 \\
    \mb{r}_{z_{t}} \triangleq & \mb{T}\mb{z}_{t} - \mu \mb{e} = 0 \\
    \mb{r}_{s_{w}} \triangleq & \mb{S}_{w} \mb{z}_{w} - \mu \mb{e} = 0 \\
    \mb{r}_{z_{l}} \triangleq & (\mb{X} - \mb{L})\mb{z}_{l} - \mu \mb{e} = 0 \\
    \mb{r}_{z_{u}} \triangleq & (\mb{U} - \mb{X})\mb{z}_{u} - \mu \mb{e} = 0
  \end{aligned}
\end{equation}
Here, the gradient of the objective function is $\mb{g} = \nabla_{x} f(\mb{x})$ and the Jacobians of the constraints are $\mb{A} = \nabla_{x} \mb{c}(\mb{x})$ and $\mb{A}_{w} = \nabla_{x} \mb{c}_{w}(\mb{x})$.

At each step of the optimization algorithm, ParOpt computes an update $\mb{p}$ to the design variables, slacks, and multipliers, based on either an inexact or an approximate Newton step, which can be written as follows:
%
\begin{equation*}
  \mb{K} \mb{p} = - \mb{r},
\end{equation*}
where $\mb{K}$ is either the exact Jacobian or an approximate Jacobian of the perturbed KKT system~\eqref{eqn:perturbed-kkt}.
When a quasi-Newton method is used the Jacobian is approximate, and when Hessian-vector products are used the Jacobian is exact, but the linear system is solved inexactly.

In the quasi-Newton mode, the matrix $\mb{K}_{B} \approx \mb{K}$ is an approximate Jacobian due to the use of a Hessian approximation $\mb{B} \approx \mb{H} \triangleq \nabla_{x}^{2} \left(f(\mb{x}) - \mb{z}^{T}\mb{c}(\mb{x}) - \mb{z}_{w}^{T}\mb{c}_{w}(\mb{x}) \right)$.
The approximate KKT matrix is
%
\begin{equation}
  \label{eqn:kkt-update}
  \mb{K}_{B}\mb{p} =
  \begin{bmatrix}
    \mb{B}    &      0 & -\mb{A}^{T} & \mb{A}_{w} &       0 &       0 &      0 & -\mb{I} & \mb{I} \\
    0 &              0 &     -\mb{I} &          0 &       0 & -\mb{I} &      0 &      0 &  0 \\
    \mb{A}    & \mb{I} &           0 &         0 & -\mb{I} &       0 &      0 &       0 & 0 \\
    \mb{A}_{w} &      0 &           0 &         0 &      0 &        0 & -\mb{I} &       0 & 0 \\
    0 &              0 &       \mb{S} &        0 & \mb{Z} &        0 &       0 &       0 & 0 \\
    0 &      \mb{Z}_{t} &           0 &        0 &       0 &   \mb{T} &       0 &       0 & 0 \\
    0 &              0 &                    0 & \mb{S}_{w} &      0 &       0 & \mb{Z}_{w} &  0 & 0 \\
    \mb{Z}_{l}  &     0 &           0 &          0 &      0 &       0 & 0 & (\mb{X} - \mb{L}) & 0 \\
    -\mb{Z}_{u} &     0 &          0 &          0 &      0 &       0 & 0 & 0 & (\mb{U} - \mb{X}) \\
  \end{bmatrix}
  \begin{bmatrix}
    \mb{p}_{x} \\
    \mb{p}_{t} \\
    \mb{p}_{z} \\
    \mb{p}_{z_{w}} \\
    \mb{p}_{s} \\
    \mb{p}_{z_{t}} \\
    \mb{p}_{s_{w}} \\
    \mb{p}_{z_{l}} \\
    \mb{p}_{z_{u}}
  \end{bmatrix} = -\mb{r}.
\end{equation}

ParOpt uses quasi-Newton Hessian approximations based either on compact limited-memory BFGS or compact limited-memory SR1 updates~\citep{Byrd:1994:quasi-Newton-LBFGS}. 
Compact representations of limited-memory quasi-Newton approximations take the form
%
\begin{equation*}
  \mb{B} = b_0\mb{I} - \mb{W}\mb{M}\mb{W}^{T},
\end{equation*}
where $b_0$ is a scalar, $\mb{M}$ is a small matrix and $\mb{W}$ is a matrix with a small number of columns that is stored as a series of vectors.
The form of these matrices depends on whether the limited-memory BFGS or SR1 technique is used. 
An exact solution to the update step $\mb{p}$ can be obtained by using the compact representation in conjunction with the Sherman-Morrison-Woodbury formula.

ParOpt approximately solves the perturbed KKT equations~\eqref{eqn:perturbed-kkt} for a sequence of barrier parameters $\mu_{k}$ such that $\mu_{k} \rightarrow 0$ for $k \rightarrow \infty$. 
ParOpt uses a monotone approach~\citet{Fiacco:McCormick:1990} in which the barrier parameter is maintained at a fixed value and reduced only after a barrier-problem convergence criterion is satisfied.
The barrier parameter criterion is that the infinity norm of the solution vector must be reduced below a factor of the barrier parameter itself
%
\begin{equation}
  \label{eqn:barrier-stopping-criterion}
  ||\mb{r}||_{\infty}  \le 10 \mu_{k}.
\end{equation}
After the barrier criterion is satisfied, the parameter is modified using the expression $\mu_{k+1} \leftarrow \min \{ \; \theta \mu_{k},\; \mu_{k}^{\beta}\;\}$ for $\beta \in (1, 2]$.

\subsection{Merit function and line search}

The interior-point method implemented in ParOpt uses a line search method that guarantees a sufficient decrease of a merit function at each iteration.
The line search is based on the following $\ell_{2}$ merit function:
%
\begin{equation}
  \label{eqn:merit-function}
  \begin{aligned}
    \phi(\alpha) = & \varphi(\mb{x} + \alpha\mb{p}_{x}^{s}, \mb{s} + \alpha\mb{p}_{s}^{s}, \mb{s}_{w}^{s} + \alpha\mb{p}^{s}_{s_{w}}; \mu) + \\
    & \nu ||\mb{c}(\mb{x} + \alpha\mb{p}_{s}^{s}) - \mb{s} + \mb{t}  - \alpha (\mb{p}_{s}^{s} - \mb{p}_{t}^{s})||_{2} +
    \nu ||\mb{c}_{w}(\mb{x} + \alpha\mb{p}^{s}_{s_{w}}) - \mb{s}_{w} - \alpha\mb{p}_{s_{w}}^{s}||_{2},
  \end{aligned}
\end{equation}
%
where $\mb{p}^{s}$ is the KKT update vector $\mb{p}$ scaled to ensure that the primal variables remain strictly within the feasible region and so that the dual variables remain positive.
The penalty parameter $\nu$ is selected to ensure a sufficiently negative descent direction, such that $\phi'(0)$ is sufficiently negative~\citep{Nocedal.Wright}. 
At each step ParOpt uses a line search that seeks a point that satisfies the Armijo sufficient decrease condition:
%
\begin{equation*}
  \phi(\alpha) < \phi(0) + c_{1} \alpha \phi'(0),
\end{equation*}
where we typically choose $c_{1} = 10^{-3}$. If a step is unsuccessful, we select the next step using a quadratic interpolation based on the initial point and slope of the merit function along the search direction, as well as the most recent merit function value. 
Since $\phi'(0)$ is negative, and $\phi(\alpha) \ge \phi(0) + c_{1} \phi'(0)$, this sequence of step lengths is decreasing.

To ensure that the design variables remain within bounds and that the dual and slack variables remain sufficiently positive, ParOpt uses a fraction-to-the-boundary rule, such that
%
\begin{equation*}
  \begin{aligned}
    \alpha_{x} & = \max\left\{ \alpha \in (0, 1]\;|\; 
      \mb{x} + \alpha \mb{p}_{x} - \mb{l} \ge (1 - \tau)(\mb{x} - \mb{l}) \right\}, \\
      %
      \alpha_{z} & = \max\left\{ \alpha \in (0, 1]\;|\; 
      \mb{z} + \alpha \mb{p}_{z} \ge (1 - \tau)\mb{z} \right\},
  \end{aligned}
\end{equation*}
%
with analogous expressions for the remaining components of the step length vector $\mb{p}$. 
The $\alpha_{x}$ and $\alpha_{z}$ parameters are then used to compute the step $\mb{p}^{s}$ such that $\mb{p}_{x}^{s} = \alpha_{x} \mb{p}_{x}$ and $\mb{p}_{z} = \alpha_{z}\mb{p}_{z}$.
Note that $\alpha_{x}$ is the step length for the design and slack variables, and $\alpha_{z}$ is the step length for the Lagrange multipliers. 
Following~\citet{Wachter:2006:IPOPT}, ParOpt sets the parameter $\tau$ as follows
%
\begin{equation*}
  \tau = \max(0.95, 1 - \mu).
\end{equation*}
To avoid situations in which there is a large discrepancy between the step lengths, a check is imposed on $\alpha_{x}$ and $\alpha_{z}$ such that if $\alpha_{x} >> \alpha_z$, ParOpt truncates the difference between the step lengths such that
%
\begin{equation*}
  \alpha_x = \max(\min(\alpha_x, 100\alpha_z), \alpha_z/100),
\end{equation*}
otherwise if $\alpha_z > \alpha_{x}$, we set:
\begin{equation*}
  \alpha_z = \max(\min(\alpha_z, 100\alpha_x), \alpha_x/100).
\end{equation*}
Note that this modification only has an effect if the difference in step lengths exceeds $100$. 
This modification does not interfere with the asymptotic convergence behavior of the algorithm and enables faster recovery from poor steps early in the optimization.

\subsection{Solving the approximate KKT system with a compact quasi-Newton Hessian}

Within ParOpt, the single most computationally expensive operation at each iteration of the optimization algorithm is the solution of the linearized KKT system obtained from the perturbed KKT conditions~\eqref{eqn:perturbed-kkt}. 
The following section presents an overview of the methods used to solve this linear system in a computationally efficient manner in parallel. 

To derive the proposed solution procedure, we first express the linearized KKT matrix as a combination of two matrices which take the form:
%
\begin{equation}
  \label{eqn:ip-newton-update}
  \mb{K}_{B} \mb{p} = \left[ \mb{K}_{0} + \mb{Y} \mb{M} \mb{Y}^{T} \right] \mb{p} = - \mb{r}
\end{equation}
where the matrix $\mb{Y}$ is
%
\begin{equation*}
  \mb{Y}^{T} = \begin{bmatrix} \mb{W}^T & 0 & 0 & 
    0 & 0 & 0 & 0 & 0 & 0 \end{bmatrix}, 
\end{equation*}
and the matrices $\mb{W}$ and $\mb{M}$ are from the compact BFGS representation.
Note that the terms in $\mb{K}_{0}$ represent the diagonal term $b_{0}$ from the compact BFGS representation and all other first-order terms from the linearized KKT system.

An exact solution to the linear system~\eqref{eqn:ip-newton-update} can be obtained using the Sherman--Morrison--Woodbury formula.  
This formula leads to the following expression for the update $\mb{p}$:
\begin{equation*}
  \mb{p} = \mb{K}^{-1}_{0} \mb{Y} \mb{C}^{-1} \mb{Y}^{T} \mb{K}_{0}^{-1} \mb{r} - \mb{K}_{0}^{-1} \mb{r}  
\end{equation*}
where the matrix $\mb{C} \in \mathbb{R}^{2m \times 2m}$ is given as
follows:
\begin{equation*}
  \mb{C} = \mb{Y}^{T} \mb{K}_{0}^{-1}\mb{Y} - \mb{M}.
\end{equation*}
A solution of the linear system~\eqref{eqn:ip-newton-update} can be obtained from the solution of $2m+1$ linear systems of the form $\mb{K}_{0}\mb{y} = \mb{b}$.  
Furthermore, if the matrix $\mb{Y}$ is stored as a series of column vectors, then the operations required to compute the solution consist of operations with small matrices of size $\mathcal{O}(m)$, parallel vector-vector products, and the application of $\mb{K}_{0}^{-1}$. 
Since vector-vector operations parallelize efficiently for distributed vectors, and the small matrix operations normally constitute a small contribution to the overall computational time, we concentrate on the parallel solution of systems of the form $\mb{K}_{0} \mb{p} =
\mb{b}$. 
Note that this refer to the linear system $\mb{K}_{0} \mb{p} = \mb{b}$ as the diagonal KKT matrix since the Hessian term in the matrix $\mb{K}_{0}$ is replaced by a diagonal matrix, $\mb{B} = b_{0}\mb{I}$.

\subsection{Parallel solution of the diagonal KKT system}

The diagonal KKT system $\mb{K}_{0} \mb{p} = \mb{b}$ is solved in parallel through a series of variable eliminations. 
In general, this method can be susceptible to numerical cancellation, however, experience has shown that this method produces remarkably accurate steps, even for very small values of the barrier parameter.  
This can be attributed to the structure of the constraint Jacobians.
The sequence of variable eliminations produces a linear system for the Lagrange multipliers of the dense constraints.  
The Schur-complement matrices that are produced during the elimination process can be precomputed and stored.
The computations during the elimination process can be reduced to a sequence of vector-vector operations and can therefore be implemented efficiently in parallel. 
Since each operation can be performed in parallel, with a small number of dense matrix operations on matrices of size $\mathcal{O}(m)$, the entire solution procedure scales efficiently.

The solution procedure begins by obtaining the solution for the slack variables and lower and upper Lagrange multiplier bound variables as follows:
%
\begin{equation}
  \label{eqn:slack-var-elim}
  \begin{aligned}
    \mb{p}_{z_{l}} &= (\mb{X} - \mb{L})^{-1}(\mb{b}_{z_{l}} - \mb{Z}_{l} \mb{p}_{x} ), \\
    \mb{p}_{z_{u}} &= (\mb{U} - \mb{X})^{-1}(\mb{b}_{z_{u}} + \mb{Z}_{u} \mb{p}_{x} ), \\
    \mb{p}_{s} &= \mb{Z}^{-1}(\mb{b}_{s} - \mb{S}\mb{p}_{z}), \\
    \mb{p}_{t} &= \mb{Z}_{t}^{-1}(\mb{b}_{z_{t}} + \mb{T}(\mb{p}_{z} + \mb{b}_{t}))\\
    \mb{p}_{s_{w}} &= \mb{Z}_{w}^{-1}(\mb{b}_{s_{w}} - \mb{S}_{w} \mb{p}_{z_{w}}),
  \end{aligned}
\end{equation}
Next, using the first three equations gives
\begin{equation}
  \label{eqn:kkt-first-elim}
  \begin{aligned}
    b_{0} \mb{p}_{x} - \mb{A}^{T}\mb{p}_{z} - \mb{A}_{w}^{T}\mb{p}_{z_{w}} - \mb{p}_{z_{l}} + \mb{p}_{z_{u}} & = \mb{b}_{x}, \\
    \mb{A} \mb{p}_{x} - \mb{p}_{s} + \mb{p}_{t} &= \mb{b}_{z}, \\
    \mb{A}_{w} \mb{p}_{x} - \mb{p}_{s_{w}} & = \mb{b}_{z_{w}}.
  \end{aligned}
\end{equation}
Substituting the expressions for the slack and Lagrange multiplier updates~\eqref{eqn:slack-var-elim} into the expression for the first three linearized KKT conditions~\eqref{eqn:kkt-first-elim} yields the following
%
\begin{equation}
  \label{eqn:kkt-x-lagrange}
  \begin{aligned}
    \mb{D} \mb{p}_{x} - \mb{A}^{T}\mb{p}_{z} - \mb{A}_{w}^{T}\mb{p}_{z_{w}} &= 
    \mb{b}_{x} + (\mb{X} - \mb{L})^{-1}\mb{b}_{z_{l}} - (\mb{U} - \mb{X})^{-1}\mb{b}_{z_{u}}, \\
    %
    \mb{A}\mb{p}_{x} + (\mb{Z}^{-1}\mb{S} + \mb{Z}_{t}^{-1}\mb{T}) \mb{p}_{z} &= \mb{b}_{z} + \mb{Z}^{-1}\mb{b}_{s} - \mb{Z}_{t}^{-1}(\mb{b}_{z_{t}} + \mb{T}\mb{b}_{t})\\
    \mb{A}_{w} \mb{p}_{x} + \mb{Z}_{w}^{-1}\mb{S}_{w} \mb{p}_{z_{w}} & = \mb{b}_{z_{w}} + \mb{Z}_{w}^{-1}\mb{b}_{s_{w}} \\
    \end{aligned}
\end{equation}
where the diagonal matrix $\mb{D}$ is defined as follows:
\begin{equation*}
  \mb{D} = \left[ b_{0} \mb{I} + (\mb{X} - \mb{L})^{-1}\mb{Z}_{l} + (\mb{U} - \mb{X})^{-1}\mb{Z}_{u} \right].
\end{equation*}
%
Finally, $\mb{p}_{x}$ can be eliminated for $\mb{p}_{z}$ and $\mb{p}_{z_{w}}$ as follows
\begin{equation}
  \begin{aligned}
    (\mb{Z}_{w}^{-1}\mb{S}_{w} + \mb{A}_{w}\mb{D}^{-1}\mb{A}_{w}^{T})\mb{p}_{z_{w}} + \mb{A}_{w}\mb{D}^{-1}\mb{A}^{T}\mb{p}_{z} &= \mb{d}_{z_{w}}, \\
    \mb{A}\mb{D}^{-1}\mb{A}_{w}^{T} \mb{p}_{z_{w}} + (\mb{Z}^{-1}\mb{S} + \mb{Z}_{t}^{-1}\mb{T} + \mb{A}\mb{D}^{-1}\mb{A}^{T})\mb{p}_{z} &= \mb{d}_{z}, \\
  \end{aligned}
\end{equation}
%
The right hand sides, $\mb{d}_{z_{w}}$ and $\mb{d}_{z}$ are 
\begin{equation*}
  \begin{aligned}
    \mb{d}_{z_{w}} &\triangleq \mb{b}_{z_{w}} + \mb{Z}_{w}^{-1}\mb{b}_{s_{w}} - \mb{A}_{w}\mb{D}^{-1}\left(\mb{b}_{x} + (\mb{X} - \mb{L})^{-1}\mb{b}_{z_{l}} - (\mb{U} - \mb{X})^{-1}\mb{b}_{z_{u}} \right), \\
    \mb{d}_{z} &\triangleq \mb{b}_{z} + \mb{Z}^{-1}\mb{b}_{s} - \mb{Z}_{t}^{-1}(\mb{b}_{z_{t}} + \mb{T}\mb{b}_{t}) - \mb{A}\mb{D}^{-1}\left(\mb{b}_{x} + (\mb{X} - \mb{L})^{-1}\mb{b}_{z_{l}} - (\mb{U} - \mb{X})^{-1}\mb{b}_{z_{u}} \right). \\
  \end{aligned}
\end{equation*}
%
Finally, defining the matrix $\mb{E} \triangleq (\mb{Z}_{w}^{-1}\mb{S}_{w} + \mb{A}_{w}\mb{D}^{-1}\mb{A}_{w}^{T})$, which is block diagonal, and introducing $\mb{F} \triangleq \mb{A}\mb{D}^{-1}\mb{A}_{w}^{T}$, gives the following equation
\begin{equation*}
  \left[\mb{Z}^{-1}\mb{S} + \mb{Z}_{t}^{-1}\mb{T} + \mb{A}\mb{D}^{-1}\mb{A}^{T} - \mb{F}\mb{E}^{-1}\mb{F}^{T} \right]\mb{p}_{z} = \mb{d}_{z} - \mb{E}^{-1} \mb{d}_{z_{w}}
\end{equation*}

This solution procedure is invoked each time a solution of the form $\mb{K}_{0}\mb{p} = \mb{b}$ is required. 

\subsection{Inexact Hessian-vector product mode}

ParOpt can also use Hessian-vector products to accelerate convergence.
This method is designed for convex optimization problems where the Hessian is positive semi-definite and the reduced Hessian is positive definite.
This solution phase is entered only after the residuals of the KKT equations are satisfied to a user-specified tolerance.
The exact Hessian phase employs an inexact Newton--Krylov method driven with the Eisenstat--Walker forcing parameters.
The inexact solution of the lineaized KKT system is obtained using right-preconditioned GMRES.
The convergence criteria within GMRES is modified to include conditions that enforce a descent direction for a line search method.
The preconditioner for the system of equations is the quasi-Newton approximation $\mb{K}_{B}$.
The product of the Jacobian and precondition is
%
\begin{equation}
  \begin{aligned}
    \mb{K}\mb{K}_{B}^{-1} & = \left(\mb{K}_{B} + 
    \mb{N}\left(\mb{H} - \mb{B}\right)\mb{N}^{T}\right)\mb{K}_{B}^{-1} \\
    %
    & = \mb{I} + \mb{N} \left(\mb{H} - \mb{B} \right) \mb{N}^{T} \mb{K}_{B}^{-1}
  \end{aligned}
\end{equation}
Here $\mb{N}$ is a matrix that consists of an identity in the $\mb{x}$-component, and zero everywhere else such that
%
\begin{equation*}
  \mb{N}^{T} = \begin{bmatrix} \mb{I} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \end{bmatrix}
\end{equation*}
The right-preconditioned operator $\mb{K}\mb{K}_{B}^{-1}$ only modifies the $\mb{x}$-components of the output vector directly. 
Since the Krylov subspace within GMRES is
\begin{equation*}
  \mathcal{K}_{m}(\mb{K}\mb{K}_{B}^{-1}, \mb{r}) = \text{span} \left\{\mb{r}, \mb{K}\mb{K}_{B}^{-1}\mb{r}, \left(\mb{K}\mb{K}_{B}^{-1}\right)^{2} \mb{r}, \ldots, \left(\mb{K}\mb{K}_{B}^{-1}\right)^{m-1} \mb{r} \right\},
\end{equation*} 
all vectors in the GMRES algorithm consist of different $\mb{x}$-component values, while all remaining components are scalar multiples of $\mb{r}$. 
This property can be used to reduce the memory requirements of GMRES by storing only the $\mb{x}$-components of each vector and a scalar for all remaining components.
Using this approach, the full vector $\hat{\mb{v}}_{i}$ is stored as a pair $(\mb{v}_{i}, \alpha_{i})$ which can be extracted as
%
\begin{equation*}
  \hat{\mb{v}}_{i} = \mb{N}\mb{v}_{i} + \alpha_{i} (\mb{I} - \mb{N}\mb{N}^{T})\mb{r}
\end{equation*}
Using this representation, the dot product of two vectors $\hat{\mb{v}}_{1}$ and $\hat{\mb{v}}_{2}$ stored as $(\mb{v}_{1}, \alpha_{1})$ and $(\mb{v}_{2}, \alpha_{2})$ can be expressed as
%
\begin{equation*}
  \begin{aligned}
    \hat{\mb{v}}_{1}^{T}\hat{\mb{v}}_{2} & = 
    \mb{v}_{1}^{T} \mb{N}^{T}\mb{N}\mb{v}_{2} + 
    \alpha_{1}\alpha_{2} \mb{r}^{T} \left(\mb{I} - \mb{N} \mb{N}^{T}\right)\mb{r} \\
      &= \mb{v}_{1}^{T} \mb{v}_{2} + \beta_{r} \alpha_{1}\alpha_{2}
  \end{aligned}
\end{equation*}
where $\beta_{r} \triangleq \mb{r}^{T} \left(\mb{I} - \mb{N} \mb{N}^{T}\right)\mb{r}$.

When using a line search method, it is necessary to obtain a descent direction.
This is guaranteed for the methods which employ a quasi-Newton approximation by maintaining a positive-definite Hessian and exactly satisfying the constraint condition $\mb{A}\mb{p}_{x} = -\mb{c}$ (omitting the slack variables).
However, in the exact Newton method a descent direction is not guaranteed.
When the objective is convex, the Hessian is positive semi-definite and a descent direction can be found if the equations are solved to sufficient precision. 
Within the present algorithm, the convergence criteria is modified within GMRES to require a sufficiently accurate solution that satisfies a descent criteria.
A penalty parameter $\nu$ for the exact $\ell_{2}$ merit function can be found if either 
\begin{equation}
  \label{eqn:descent-function}
  \mb{p}_{x}^{T} \nabla_{x} f < 0,
\end{equation}
or 
\begin{equation}
  \label{eqn:descent-constraint}
  \mb{c}^{T}\mb{A} \mb{p}_{x} \le - \gamma ||\mb{c}||_{2}^2,
\end{equation}
for some $0 < \gamma < 1$.
Therefore, if either condition is satisfied we will have a descent direction.

While it is possible to build the full solution at each GMRES iteration, and check the criteria~\eqref{eqn:descent-function} and~\eqref{eqn:descent-constraint}, it is more efficient to modify GMRES to evaluate these criteria indirectly.
GMRES works by building an orthogonal subspace $\mb{V}_{k} \in \mathbb{R}^{n \times k}$ using Arnoldi's method that satisfies the following equation
\begin{equation*}
  \mb{K}\mb{K}_{B}^{-1}\mb{V}_{k} = \mb{V}_{k+1} \bar{\mb{H}}_{k+1}
\end{equation*}
where the approximate solution is $\mb{p} = \mb{K}_{B}^{-1}\mb{V}_{k} \mb{y}_{k}$ where $\mb{y}_{k}$ is obtained from the solution of the least-squares problem
\begin{equation*}
  \mb{y}_{k} = \text{arg} \min_{y} || \bar{\mb{H}}_{k} \mb{y} - \beta \mb{e}_{1} ||_{2}.
\end{equation*}
At each iteration, we compute the action of the matrix $\mb{w} \leftarrow \mb{K}\mb{K}_{B}^{-1}\mb{v}$ by first computing the intermediate vector $\mb{z} = \mb{K}_{B}^{-1} \mb{v}$.
Next, the $\mb{x}$-components of the output vector $\mb{w}$ are obtained by computing
%
\begin{equation*}
  \mb{w}_{x} = \mb{v}_{x} + (\mb{H} - \mb{B})\mb{z}_{x}.
\end{equation*}
Before discarding the intermediate vector $\mb{z}$, we compute the directional derivatives of the objective and the $\ell_{2}$ norm of the constraints, respectively as follows
\begin{equation*}
  a_{k} = \mb{z}_{x}^{T} \nabla f \qquad\qquad b_{k} = \mb{z}_{x}^{T} \mb{A}^{T} \mb{c}.
\end{equation*}
%
Based on these values, the directional derivative of the inexact solution at iteration $k$ of GMRES can be evaluated as
\begin{equation*}
  \begin{aligned}
    \mb{p}_{x}^{T} \nabla f &= \mb{y}_{k}^{T}\mb{a}_{k}, \\
    \mb{p}_{x}^{T} \mb{A}^{T} \mb{c} &= \mb{y}_{k}^{T}\mb{b}_{k}.
  \end{aligned}
\end{equation*}
%
These quantities can be computed inexpensively by evaluating $\mb{y}_{k}$ at every iteration of GMRES, rather than after the final iteration.
If GMRES fails to find an inexact solution that is also a descent direction, we revert back to the quasi-Newton step which is guaranteed to produce a descent direction.

\begin{figure}
  \begin{algorithmic}
    \State{GMRES(m, $\gamma$): Inexactly solve $\mb{K}\mb{p} = \mb{b}$ while ensuring $\mb{p}$ is a descent direction}
    \State{Given the relative stopping tolerance $\epsilon_{r}$ and the descent fraction $\gamma$}
    \State{Evaluate $\beta = ||\mb{b}||_{2}$ and $\beta_{r} = \mb{b}^{T} \left(\mb{I} - \mb{N} \mb{N}^{T}\right)\mb{b}$}
    \State{Set $\mb{v}_{1} = \beta^{-1}\mb{b}$ and $k = 1$}
    %
    \While{$k \le m$}
      \State{Compute $\mb{z} = \mb{K}_{B}^{-1} \mb{v}_{k}$}
      \State{Compute $a_{k} = \mb{z}^{T}\mb{N}^{T}\nabla_{x} f$}  
      \State{Compute $b_{k} = \mb{z}^{T}\mb{N}^{T}\mb{A}^{T}\mb{c}$}
      \State{Set $\mb{a}_{k} = (\mb{a}_{k-1}, a_{k})$ and $\mb{b}_{k} = (\mb{b}_{k-1}, b_{k})$}
      \State{Compute $\mb{w} \leftarrow \mb{K}\mb{z}$}
      \State{Compute $\mb{V}_{k+1} \leftarrow MGS(\mb{V}_{k}, \mb{w})$}
      \State{Solve $\mb{y}_{k} = \text{arg} \min_{\mb{y}} || \bar{\mb{H}}_{k} \mb{y} - \beta \mb{e}_{1}||_{2}$}
      \If{$\mb{y}_{k}^{T}\mb{a}_{k} < 0$ or $\mb{y}_{k}^{T}\mb{b}_{k} \le -\gamma ||\mb{c}||_{2}^{2}$}
        \If{$||\bar{\mb{H}}_{k} \mb{y} - \beta \mb{e}_{1}||_{2} < \epsilon_{r} \beta$}
          \State{Successfully found inexact solution satisfying descent direction criteria.}
          \State{\textbf{break}}
        \EndIf
      \EndIf
      \State{$k \leftarrow k + 1$}
    \EndWhile
    \State{Set $\mb{z} = \mb{V}_{k} \mb{y}_{k}$}
    \State{Compute $\mb{p} = \mb{K}_{B}^{-1}\mb{z}$}
    \State{Verify $\mb{p}^{T}\mb{N}\nabla_{x} f < 0$ or $\mb{p}^{T}\mb{N}^{T} \mb{A}^{T}\mb{c} \le -\gamma ||\mb{c}||_{2}^{2}$}
  \end{algorithmic}
  \caption{GMRES with descent direction convergence criteria}
\end{figure}

\section{Implementation details}

The two main classes need by users of ParOpt are \texttt{ParOptProblem} and \texttt{ParOpt}.
These classes are both accessible using Python through a Cython wrapper.
The \texttt{ParOptProblem} class is an abstract base class that is designed to implement the functions needed to solve an optimization problem. 
It is responsible for evaluating functions and constraints as well as allocating parallel design and sparse constraint vectors.
The python-level implementation of this class uses a default vector implementation where the components of the vector are distributed across all processors in the communicator provided to ParOpt.

The \texttt{ParOpt} class itself is responsible for optimizing the problem.
All options are set through public member-access functions that can be called from the python or C++ interfaces.
These functions will be described below.

\subsection{ParOptProblem class methods}

The constructor for the \texttt{ParOptProblem} class takes the following form:
%
\begin{verbatim}
ParOptProblem( MPI_Comm _comm,
               int _nvars, int _ncon,
               int _nwcon, int _nwblock );
\end{verbatim}

The arguments to this constructor are:
\begin{itemize}
\item \texttt{comm} is the MPI communicator for the problem. 
This communicator will be passed to the corresponding \texttt{ParOpt} optimizer class.

\item \texttt{nvars}: The local number of design variables that are owned by this processor.

\item \texttt{ncon}: The global number of dense constraints.

\item \texttt{nwcon}: The local number of sparse constraints that are owned by this processor

\item \texttt{nwblock}: The size of the block in block-diagonal matrix formed from the weighting constraints $\mb{A}_{w}^{T}\mb{A}_{w}$. This size must be the same on all processors.
\end{itemize}

The following functions are used to specify the structure of the design problem:
\begin{enumerate}
\item \texttt{int isDenseInequality()}: Are the dense constraints inequality or equality constraints?
\item \texttt{int isSparseInequality()}: Are the sparse constraints inequality or equality constraints?
\item \texttt{int useLowerBounds()}: Should the optimizer use the lower bound constraints?
\item \texttt{int useUpperBounds()}: Should the optimizer use the upper bound constraints?
\end{enumerate}

\subsubsection{ParOptProblem evaluation functions}

The following evaluation member functions must be defined. Their arguments are omitted here, but can be found
in the file \texttt{src/ParOptProblem.h}.
\begin{enumerate}
\item \texttt{getVarsAndBounds}: Get the design variables at the starting point as well as the lower and upper bounds for the variables. This is called when \texttt{ParOpt} is initialized. 

\item \texttt{int evalObjCon}: Given the design variable values, evaluate the objective and dense constraint functions. This returns a fail flag. When the fail flag is non-zero, the function has failed.

\item \texttt{int evalObjConGradient}: Given the design variable values, evaluate the objective and constraint gradients. The objective is a single \texttt{ParOptVec} instance. The dense constraint Jacobian is an array of \texttt{ParOptVec} instances. This function also returns a fail flag. 

\item \texttt{int evalHvecProduct}: Given the design variable values, the multipliers for the dense and sparse constraints, and a direction in the design space, compute the Hessian-vector product. Note that this function combines computations using the sparse and dense constraints. This function also returns a fail flag.
\end{enumerate}

\subsubsection{ParOptProblem sparse constraint functions}

The following functions are used exclusively for the sparse constraints
%
\begin{enumerate}
\item \texttt{evalSparseCon}: Given the design variables, evaluate the sparse constraints.

\item \texttt{addSparseJacobian} Given a scalar, the design variables, and an input direction vector the size of the number of design variables, compute the scaled Jacobian-vector product of the sparse constraints.

\item \texttt{addSparseJacobianTranspose} Given a scalar, the design variables, and an input direction vector the same size as the number of sparse constraints, compute the scaled transpose Jacobian-vector product of the sparse constraints. This function is required for computing the product $\mb{A}_{w}^{T}\mb{z}_{w}$.

\item \texttt{addSparseInnerProduct} Given a scalar, the design variable vector, and the diagonal of a square matrix the size of the number of sparse constraints, compute add the product, $\mb{D} \leftarrow \mb{D} + \alpha \mb{A}_{w} \mb{S}\mb{A}_{w}^{T}$.
\end{enumerate}


ParOpt implements a default vector class. 
It is often convenient or necessary to override this class for certain design problems.
ParOptProblem can override the default implementation if the following two functions are provided:
\begin{enumerate}
\item \texttt{ParOptVec *createDesignVec()}: Create a vector with the right type and shape to store design variables/objective gradients.
\item \texttt{ParOptVec *createConstraintVec()}: Create a vector with the right type and shape to store a sparse constraint vector
\end{enumerate}  

The function \texttt{void writeOutput( int iter, ParOptVec *x )} can be overridden to write out design-dependent data at a specified frequency.
  
\subsection{ParOpt class methods}

The prototype for the constructor for the \texttt{ParOpt} class is:
%
\begin{verbatim}
ParOptInteriorPoint( ParOptProblem *_prob,
                     int _max_qn_subspace,
                     QuasiNewtonType qn_type=BFGS,
                     double _max_bound_val=1e20 );
\end{verbatim}
%
The \texttt{ParOptProblem} defines the problem size, structure, and serves as a factory for generating problem-specific vectors.
The \texttt{max\_qn\_subspace} denotes the maximum size of the quasi-Newton subspace while \texttt{qn\_type} denotes the type of quasi-Newton method. Note that these parameters must be specified at initialization.

The following are the primary member functions used in \texttt{ParOpt}:
\begin{enumerate}
\item \texttt{int optimize( const char *checkpoint=NULL )} Begin the optimization algorithm and use the given checkpoint file. This binary file can be used to restart the optimization from the last iterate with the same design variables, barrier parameter and multipliers. This does not retain the same set of optimization parameters.

\item \texttt{void checkGradients( double dh )}: Check the gradients of the objective and constraints as well as the Hessian-vector products if they are defined. This call should be made when debugging the optimization problem. This check uses finite-difference or complex-step method if the code is compiled in complex mode.

\item \texttt{void setMaxMajorIterations( int iters )}: Set the maximum number of iterations to use during the optimization

\item \texttt{void setAbsOptimalityTol( double tol )}: Set the absolute optimality tolerance used within ParOpt. This is an absolute tolerance on the $\ell_{\infty}$ norm of the KKT conditions.

\item \texttt{void setRelFunctionTol( double tol )}: Set the relative function tolerance stopping criterion. ParOpt will terminate a barrier problem when the decrease in the objective function value falls below the specified tolerance. This defaults to zero, in which case the criteria is never satisfied.

\item \texttt{void setInitBarrierParameter( double mu )}: Set the initial barrier parameter

\item \texttt{void setMaxLineSearchIters( int iters )}: Set the maximum number of iterations before the line search fails

\item \texttt{void setBacktrackingLineSearch( int truth )}: Use a backtracking line search rather than 

\item \texttt{void setArmijoParam( double c1 )}: Set the Armijo parameter in the line search acceptance criteria (first Wolfe condition).

\item \texttt{void setOutputFrequency( int freq )}: Set the output iteration frequency. This is the frequency at which the \texttt{writeOutput} function is called in \texttt{ParOptProblem}.

\item \texttt{void setOutputFile( const char *filename )}: Set the output file name for the optimization summary file. This file is flushed at the same frequency as the \texttt{writeOutput} call.

\item \texttt{int writeSolutionFile( const char *filename )}: Write a binary solution file containing the multipliers/design variables

\item \texttt{int readSolutionFile( const char *filename )}: Read a binary solution file and set the variables internally. This requires that the problem sizes are exactly the same as the original problem. A different number of processors may be used if the design variables and multipliers are numbered consistently. 
\end{enumerate}

\subsubsection{Advanced ParOpt functions}

The following are more advanced options/member functions needed for specific applications

\begin{enumerate}
\item \texttt{getInitMultipliers}: Get the multiplier vectors stored internally in ParOpt. This is needed if you have specific knowledge of what the best multiplier estimates should be before starting the optimization algorithm.

\item \texttt{getOptimizedPoint}: Get the optimized point including the design variables and multipliers from ParOpt. This can be called at any time to return the current approximate solution from ParOpt. 

\item \texttt{setMaxAbsVariableBound( doublee max\_bound )}: Set the maximum absolute value of the bound variables in the problem. Components of the $\mb{l}$ and $\mb{u}$ vector that are specified beyond this limit will be ignored by ParOpt.

\item \texttt{void setInitStartingPoint( int init )}: Tells ParOpt whether it should attempt to guess the initial multipliers or use the values that are set in ParOpt, possibly with a call to getInitMultipliers.

\item \texttt{void setBarrierFraction( double frac )}: Set the fraction applied during the Fiacco--McCormick barrier parameter update scheme
\item \texttt{void setBarrierPower( double power )}: Set the barrier power applied during the Fiacco--McCormick barrier parameter update scheme

\item \texttt{double getBarrierParameter()}: Get the barrier parameter
\item \texttt{ParOptScalar getComplementarity()}: Compute the average complementarity product at the current design point

\item \texttt{void setUseLineSearch( int truth )}: Set a flag to indicate whether a line search will be performed. If false, then the new point is automatically accepted once the fraction-to-the-boundary rule is applied to the step (to ensure the multipliers/slacks remain positive). 
 
\item \texttt{void setHessianResetFreq( int freq )}: Set the frequency at which the Hessian will be reset to a diagonal matrix
\item \texttt{void setBFGSUpdateType( LBFGS::BFGSUpdateType bfgs\_update )}: Set the type of BFGS update to use when a negative curvature condition is encountered. This will either skip or update the either damped or skip

\item \texttt{void setUseHvecProduct( int truth )}: Set the flag to either the Hessian-vector product code. This relies on the implementation of the Hessian-vector product code in the optimization class itself.

\item \texttt{void setUseQNGMRESPreCon( int truth )}: Use the BFGS or SR1 update as a preconditioner in the GMRES solver.

\item \texttt{void setNKSwitchTolerance( double tol )}: Set the optimality tolerance at which to switch to the full Newton--Krylov method.

\item \texttt{void setEisenstatWalkerParameters( double gamma, double alpha )}: Set the Eisenstat--Walker parameters for the forcing term

\item \texttt{void setGMRESSubspaceSize( int gmres\_subspace\_size )}: Set the size of the GMRES subspace to use during the optimization algorithm

\item \texttt{void setPenaltyDescentFraction( double frac )}: Set the penalty parameter 

\item \texttt{void resetQuasiNewtonHessian()}: Force a reset of the quasi-Newton Hessian approximation

\item \texttt{void resetDesignAndBounds()}: Force a reset the design variables and bounds within the problem
\end{enumerate}

\bibliographystyle{abbrvnat}
\bibliography{refs}

\end{document}
